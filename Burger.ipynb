{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1fdd7717f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the architecture as seen in : https://arxiv.org/pdf/1811.08782.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGM_layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_feature, residual = False):\n",
    "        super(DGM_layer, self).__init__()\n",
    "        self.residual = residual\n",
    "        \n",
    "        self.Z = nn.Linear(out_feature,out_feature) ; self.UZ = nn.Linear(in_features,out_feature, bias=False)\n",
    "        self.G = nn.Linear(out_feature,out_feature) ; self.UG = nn.Linear(in_features,out_feature, bias=False)\n",
    "        self.R = nn.Linear(out_feature,out_feature) ; self.UR = nn.Linear(in_features,out_feature, bias=False)\n",
    "        self.H = nn.Linear(out_feature,out_feature) ; self.UH = nn.Linear(in_features,out_feature, bias=False)\n",
    "    \n",
    "\n",
    "    def forward(self, x, s):\n",
    "        z = torch.relu(self.UZ(x)+self.Z(s))\n",
    "        g = torch.relu(self.UG(x)+self.G(s))\n",
    "        r = torch.relu(self.UR(x)+self.R(s))\n",
    "        h = torch.relu(self.UH(x)+self.H(s*r))\n",
    "        return (1 - g) * h + z*s\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGM_net(nn.Module):\n",
    "    def __init__(self, in_dim,out_dim, n_layers, n_neurons, residual = False):\n",
    "        \"\"\" in_dim is number of cordinates + 1 \n",
    "            out_dim is the number of output\n",
    "            n_layers and n_neurons are pretty self explanatory\n",
    "            make residual = true for identity between each DGM layers\n",
    "        \"\"\"\n",
    "        super(DGM_net, self).__init__()\n",
    "        self.in_dim = in_dim ; self.out_dim = out_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.residual = residual\n",
    "\n",
    "        self.first_layer = nn.Linear(in_dim, n_neurons)\n",
    "        \n",
    "        self.dgm_layers = nn.ModuleList([DGM_layer(self.in_dim, self.n_neurons,\n",
    "                                                       self.residual) for i in range(self.n_layers)])\n",
    "        self.final_layer = nn.Linear(n_neurons,out_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        s = torch.relu(self.first_layer(x))\n",
    "        for i,dgm_layer in enumerate(self.dgm_layers):\n",
    "            s = dgm_layer(x, s)\n",
    "        \n",
    "        return  self.final_layer(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time limits\n",
    "T0 = 0.0 + 1e-10    # Initial time\n",
    "T  = 1.0            # Terminal time\n",
    "\n",
    "# Space limits\n",
    "S_1 = 0.0 + 1e-10    # Low boundary\n",
    "S_2 = 1              # High boundary\n",
    "\n",
    "#viscosity limits\n",
    "V1 = 1e-2\n",
    "V2 = 1e-1\n",
    "\n",
    "#alpha ??\n",
    "al1 = 1e-2\n",
    "al2 = 1\n",
    "\n",
    "#Boundary condition for x = 0\n",
    "a1 = -1\n",
    "a2 = 1\n",
    "\n",
    "#Boundary condition for x = 1\n",
    "b1 = -1\n",
    "b2 = 1\n",
    "\n",
    "#initial condition\n",
    "def g(x, a, b) : return a + x *(b-a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of domain sampling and loss function from https://github.com/adolfocorreia/DGM/, adapted to pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler_space_time(N1, N2, N3):\n",
    "    # Sampler #1: PDE domain\n",
    "    t1 = np.random.uniform(low=T0 - 0.5*(T - T0),\n",
    "                           high=T,\n",
    "                           size=[N1,1])\n",
    "    s1 = np.random.uniform(low=S_1 - (S_2 - S_1)*0.5,\n",
    "                           high=S_2 + (S_2 - S_1)*0.5,\n",
    "                           size=[N1,1])\n",
    "\n",
    "    # Sampler #2: boundary condition x=0\n",
    "    t2 = np.random.uniform(low=T0 - 0.5*(T - T0),\n",
    "                           high=T,\n",
    "                           size=[N2,1])\n",
    "    s2 = np.zeros(shape=(N2, 1))\n",
    "    \n",
    "    # Sampler #3: boundary x=1\n",
    "    t3 = np.random.uniform(low=T0 - 0.5*(T - T0),\n",
    "                           high=T,\n",
    "                           size=[N2,1])\n",
    "    s3 = np.ones(shape=(N2, 1))\n",
    "    \n",
    "    #sampler 4 : initial condition \n",
    "    t4 = np.zeros(shape=(N3, 1))\n",
    "    s4 = np.random.uniform(low=S_1 - (S_2 - S_1)*0.5,\n",
    "                           high=S_2 + (S_2 - S_1)*0.5,\n",
    "                           size=[N3,1])\n",
    "    \n",
    "    t1=torch.tensor(t1, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    s1=torch.tensor(s1, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    t2=torch.tensor(t2, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    s2=torch.tensor(s2, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    t3=torch.tensor(t3, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    s3=torch.tensor(s3, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    t4=torch.tensor(t4, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    s4=torch.tensor(s4, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    return (t1, s1, t2, s2, t3, s3, t4, s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler_parameters(N1, N2, N3):\n",
    "    alpha1 = np.random.uniform(low=al1, high = al2, size=[N1,1])\n",
    "    alpha2 = np.random.uniform(low=al1, high = al2, size=[N2,1])\n",
    "    alpha3 = np.random.uniform(low=al1, high = al2, size=[N3,1])\n",
    "    \n",
    "    nu1 = np.random.uniform(low=V1, high = V2, size=[N1,1])\n",
    "    nu2 = np.random.uniform(low=V1, high = V2, size=[N2,1])\n",
    "    nu3 = np.random.uniform(low=V1, high = V2, size=[N3,1])\n",
    "    \n",
    "    a_1 = np.random.uniform(low=a1, high = a2, size=[N1,1])\n",
    "    a_2 = np.random.uniform(low=a1, high = a2, size=[N2,1])\n",
    "    a_3 = np.random.uniform(low=a1, high = a2, size=[N3,1])\n",
    "    \n",
    "    b_1 = np.random.uniform(low=b1, high = b2, size=[N1,1])\n",
    "    b_2 = np.random.uniform(low=b1, high = b2, size=[N2,1])\n",
    "    b_3 = np.random.uniform(low=b1, high = b2, size=[N3,1])\n",
    "    \n",
    "    alpha1 = torch.tensor(alpha1, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    alpha2 = torch.tensor(alpha2, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    alpha3 = torch.tensor(alpha3, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    \n",
    "    nu1 = torch.tensor(nu1, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    nu2 = torch.tensor(nu2, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    nu3 = torch.tensor(nu3, dtype=torch.float32, requires_grad=True).cuda()\n",
    "\n",
    "    a_1 = torch.tensor(a_1, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    a_2 = torch.tensor(a_2, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    a_3 = torch.tensor(a_3, dtype=torch.float32, requires_grad=True).cuda()\n",
    "\n",
    "\n",
    "    b_1 = torch.tensor(b_1, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    b_2 = torch.tensor(b_2, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    b_3 = torch.tensor(b_3, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    \n",
    "    return (alpha1, alpha2, alpha3, nu1, nu2, nu3, a_1, a_2, a_3, b_1, b_2, b_3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(model, S1, S2, S3, S4):\n",
    "    #Each set contain [x,t,nu,alpha,a,b]\n",
    "    \n",
    "    # Loss term #1: PDE\n",
    "    U = model(S1)\n",
    "    DU = torch.autograd.grad(U.sum(), S1, create_graph=True, retain_graph=True)[0]\n",
    "    U_t = DU[:,1]\n",
    "    U_x = DU[:,0]\n",
    "    U_xx = torch.autograd.grad(U_x.sum(), S1, create_graph = True, retain_graph=True)[0][:,0]\n",
    "    \n",
    "    f = U_t - S1[:,2]*U_xx + S1[:,3] * U * U_x\n",
    "    L1 = torch.mean(torch.pow(f,2))\n",
    "\n",
    "    # Loss term #2: boundary condition x=0\n",
    "    Ub1 = model(S2)\n",
    "    L2 = torch.mean(torch.pow(Ub1[:,0]-S2[:,4], 2))\n",
    "    \n",
    "    # Loss term #2: boundary condition x=1\n",
    "    Ub2 = model(S3)\n",
    "    L3 = torch.mean(torch.pow(Ub2[:,0]-S3[:,5], 2))\n",
    "    \n",
    "    # Loss term #3: initial/terminal condition\n",
    "    CI = g(S4[:,0], S4[:,4], S4[:,5])\n",
    "    L4 = torch.mean(torch.pow((model(S4)[:,0] - CI) ,2))\n",
    "\n",
    "    return L1, L2, L3, L4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model init and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DGM_net(6,1,6,200)\n",
    "model.cuda()\n",
    "opt = torch.optim.Adam(model.parameters(),lr = 0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(opt,gamma=0.99)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "# Number of samples\n",
    "NS_1 = 1000  #samples on domain\n",
    "NS_2 = 100  #samples for BC\n",
    "NS_3 = 100  #samples for IC\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "steps_per_sample = 10\n",
    "sampling_stages = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, num_epochs=100):\n",
    "    since = time.time()\n",
    "    model.train()\n",
    "  # Set model to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        t1, x1, t2, x2, t3, x3, t4, x4 = sampler_space_time(NS_1, NS_2, NS_3) \n",
    "        alpha1, alpha2, alpha3, nu1, nu2, nu3, a_1, a_2, a_3, b_1, b_2, b_3 = sampler_parameters(NS_1, NS_2, NS_3)\n",
    "        \n",
    "        S1 = torch.cat((x1,t1,nu1, alpha1, a_1, b_1), 1)\n",
    "        S2 = torch.cat((x2,t2,nu2, alpha2, a_2, b_2), 1)\n",
    "        S3 = torch.cat((x3,t3,nu2, alpha2, a_2, b_2), 1)\n",
    "        S4 = torch.cat((x4,t4,nu3, alpha3, a_3, b_3), 1)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        for _ in range(steps_per_sample) :\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # forward\n",
    "            L1, L2, L3, L4 = Loss(model, S1, S2, S3, S4)\n",
    "    \n",
    "            loss = L1 + L2 + L3 + L4\n",
    "            # backward + optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        epoch +=1 \n",
    "        if epoch % (num_epochs//num_epochs) == 0: print(f'epoch {epoch}, loss {loss.data}, L1 : {L1.data}')\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f\"Training finished in {time_elapsed:.2f} for {num_epochs}.\")\n",
    "    print(f\"The final loss value is {loss.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1.0830315351486206, L1 : 0.00682768365368247\n",
      "epoch 2, loss 1.059787631034851, L1 : 0.006554476451128721\n",
      "epoch 3, loss 1.1586031913757324, L1 : 0.00684357201680541\n",
      "epoch 4, loss 0.9778107404708862, L1 : 0.006685047876089811\n",
      "epoch 5, loss 1.0076286792755127, L1 : 0.006252388935536146\n",
      "epoch 6, loss 1.0590533018112183, L1 : 0.005900115240365267\n",
      "epoch 7, loss 1.0388705730438232, L1 : 0.005843978375196457\n",
      "epoch 8, loss 1.0774928331375122, L1 : 0.005365019664168358\n",
      "epoch 9, loss 1.100426197052002, L1 : 0.005127577111124992\n",
      "epoch 10, loss 1.0812653303146362, L1 : 0.00538421468809247\n",
      "epoch 11, loss 0.9620516300201416, L1 : 0.004902665037661791\n",
      "epoch 12, loss 1.0113743543624878, L1 : 0.005016155540943146\n",
      "epoch 13, loss 0.872666597366333, L1 : 0.004897871054708958\n",
      "epoch 14, loss 0.9823595285415649, L1 : 0.004773371387273073\n",
      "epoch 15, loss 0.9100465774536133, L1 : 0.004677505698055029\n",
      "epoch 16, loss 1.0025830268859863, L1 : 0.004732554778456688\n",
      "epoch 17, loss 0.9774953126907349, L1 : 0.004164079669862986\n",
      "epoch 18, loss 0.8733968138694763, L1 : 0.004314354155212641\n",
      "epoch 19, loss 0.835740327835083, L1 : 0.004185518715530634\n",
      "epoch 20, loss 0.8545113205909729, L1 : 0.003904922865331173\n",
      "epoch 21, loss 0.7894601821899414, L1 : 0.004408828914165497\n",
      "epoch 22, loss 0.8142595887184143, L1 : 0.0043859463185071945\n",
      "epoch 23, loss 0.8283017873764038, L1 : 0.004389329347759485\n",
      "epoch 24, loss 0.7993780374526978, L1 : 0.004085362423211336\n",
      "epoch 25, loss 0.8422147631645203, L1 : 0.004194679670035839\n",
      "epoch 26, loss 0.711372971534729, L1 : 0.0039092740043997765\n",
      "epoch 27, loss 0.8345780372619629, L1 : 0.00408480828627944\n",
      "epoch 28, loss 0.8383102416992188, L1 : 0.0039827413856983185\n",
      "epoch 29, loss 0.6824970841407776, L1 : 0.0037465456407517195\n",
      "epoch 30, loss 0.791685163974762, L1 : 0.0037656293716281652\n",
      "epoch 31, loss 0.8435096740722656, L1 : 0.0038751137908548117\n",
      "epoch 32, loss 0.7479859590530396, L1 : 0.0037905422504991293\n",
      "epoch 33, loss 0.6726940870285034, L1 : 0.003916412126272917\n",
      "epoch 34, loss 0.7168059945106506, L1 : 0.003779592225328088\n",
      "epoch 35, loss 0.6654284000396729, L1 : 0.004070756491273642\n",
      "epoch 36, loss 0.6710973978042603, L1 : 0.004151165951043367\n",
      "epoch 37, loss 0.6629541516304016, L1 : 0.0041376943700015545\n",
      "epoch 38, loss 0.625063419342041, L1 : 0.004918976221233606\n",
      "epoch 39, loss 0.673276424407959, L1 : 0.004950312897562981\n",
      "epoch 40, loss 0.6341339349746704, L1 : 0.005909871309995651\n",
      "epoch 41, loss 0.5154455304145813, L1 : 0.007481080014258623\n",
      "epoch 42, loss 0.5757883787155151, L1 : 0.009551540948450565\n",
      "epoch 43, loss 0.6184936165809631, L1 : 0.007340158335864544\n",
      "epoch 44, loss 0.5268630385398865, L1 : 0.009003381244838238\n",
      "epoch 45, loss 0.49637195467948914, L1 : 0.009934371337294579\n",
      "epoch 46, loss 0.47157829999923706, L1 : 0.014277498237788677\n",
      "epoch 47, loss 0.59619140625, L1 : 0.016288211569190025\n",
      "epoch 48, loss 0.4822639524936676, L1 : 0.017497550696134567\n",
      "epoch 49, loss 0.437954306602478, L1 : 0.01959856040775776\n",
      "epoch 50, loss 0.4470837116241455, L1 : 0.024457234889268875\n",
      "epoch 51, loss 0.46435099840164185, L1 : 0.017341244965791702\n",
      "epoch 52, loss 0.4315098524093628, L1 : 0.021988386288285255\n",
      "epoch 53, loss 0.38742852210998535, L1 : 0.020251452922821045\n",
      "epoch 54, loss 0.4567270874977112, L1 : 0.020463230088353157\n",
      "epoch 55, loss 0.4157664179801941, L1 : 0.026835136115550995\n",
      "epoch 56, loss 0.35820502042770386, L1 : 0.02778756618499756\n",
      "epoch 57, loss 0.4034763276576996, L1 : 0.020636070519685745\n",
      "epoch 58, loss 0.43049460649490356, L1 : 0.02287600375711918\n",
      "epoch 59, loss 0.45530974864959717, L1 : 0.027038035914301872\n",
      "epoch 60, loss 0.3635849058628082, L1 : 0.019828207790851593\n",
      "epoch 61, loss 0.42956796288490295, L1 : 0.02468283474445343\n",
      "epoch 62, loss 0.37466418743133545, L1 : 0.023001374676823616\n",
      "epoch 63, loss 0.38273370265960693, L1 : 0.028038078919053078\n",
      "epoch 64, loss 0.3717496991157532, L1 : 0.031255707144737244\n",
      "epoch 65, loss 0.37815263867378235, L1 : 0.03020760416984558\n",
      "epoch 66, loss 0.384230375289917, L1 : 0.02177456207573414\n",
      "epoch 67, loss 0.31534257531166077, L1 : 0.02732764184474945\n",
      "epoch 68, loss 0.4070722162723541, L1 : 0.0322236530482769\n",
      "epoch 69, loss 0.31119707226753235, L1 : 0.026824476197361946\n",
      "epoch 70, loss 0.31722790002822876, L1 : 0.027382299304008484\n",
      "epoch 71, loss 0.34241190552711487, L1 : 0.029323691502213478\n",
      "epoch 72, loss 0.3319213092327118, L1 : 0.030536405742168427\n",
      "epoch 73, loss 0.29891279339790344, L1 : 0.03235888481140137\n",
      "epoch 74, loss 0.33283892273902893, L1 : 0.032131366431713104\n",
      "epoch 75, loss 0.31341180205345154, L1 : 0.03357723355293274\n",
      "epoch 76, loss 0.2742632031440735, L1 : 0.03274144232273102\n",
      "epoch 77, loss 0.3475247323513031, L1 : 0.034618739038705826\n",
      "epoch 78, loss 0.25586479902267456, L1 : 0.036162663251161575\n",
      "epoch 79, loss 0.3111289143562317, L1 : 0.03470204770565033\n",
      "epoch 80, loss 0.3015576899051666, L1 : 0.03579741343855858\n",
      "epoch 81, loss 0.2645282447338104, L1 : 0.03581880405545235\n",
      "epoch 82, loss 0.30889660120010376, L1 : 0.033805761486291885\n",
      "epoch 83, loss 0.3078824281692505, L1 : 0.03907712548971176\n",
      "epoch 84, loss 0.281158983707428, L1 : 0.037843991070985794\n",
      "epoch 85, loss 0.2506820559501648, L1 : 0.03168689087033272\n",
      "epoch 86, loss 0.2836998403072357, L1 : 0.04067544639110565\n",
      "epoch 87, loss 0.31758594512939453, L1 : 0.038638655096292496\n",
      "epoch 88, loss 0.2971879839897156, L1 : 0.03227045387029648\n",
      "epoch 89, loss 0.25367456674575806, L1 : 0.030907543376088142\n",
      "epoch 90, loss 0.2670971155166626, L1 : 0.038564376533031464\n",
      "epoch 91, loss 0.23976820707321167, L1 : 0.036905437707901\n",
      "epoch 92, loss 0.21218827366828918, L1 : 0.03347373381257057\n",
      "epoch 93, loss 0.24818265438079834, L1 : 0.034596141427755356\n",
      "epoch 94, loss 0.22121639549732208, L1 : 0.039362791925668716\n",
      "epoch 95, loss 0.266380250453949, L1 : 0.034855566918849945\n",
      "epoch 96, loss 0.22443066537380219, L1 : 0.039528634399175644\n",
      "epoch 97, loss 0.2373906373977661, L1 : 0.04115227237343788\n",
      "epoch 98, loss 0.2615625858306885, L1 : 0.03862147778272629\n",
      "epoch 99, loss 0.23106050491333008, L1 : 0.03664000704884529\n",
      "epoch 100, loss 0.22317813336849213, L1 : 0.03707486018538475\n",
      "epoch 101, loss 0.2353750616312027, L1 : 0.03853469341993332\n",
      "epoch 102, loss 0.21330998837947845, L1 : 0.04149085283279419\n",
      "epoch 103, loss 0.2522878050804138, L1 : 0.03683535009622574\n",
      "epoch 104, loss 0.232722207903862, L1 : 0.04501529783010483\n",
      "epoch 105, loss 0.22526603937149048, L1 : 0.045271020382642746\n",
      "epoch 106, loss 0.18217672407627106, L1 : 0.035820771008729935\n",
      "epoch 107, loss 0.21384748816490173, L1 : 0.03752145916223526\n",
      "epoch 108, loss 0.2258787900209427, L1 : 0.04725882038474083\n",
      "epoch 109, loss 0.1877119243144989, L1 : 0.03216865658760071\n",
      "epoch 110, loss 0.21533504128456116, L1 : 0.04840575158596039\n",
      "epoch 111, loss 0.21084541082382202, L1 : 0.03728189691901207\n",
      "epoch 112, loss 0.17062124609947205, L1 : 0.029554149135947227\n",
      "epoch 113, loss 0.20832034945487976, L1 : 0.036319300532341\n",
      "epoch 114, loss 0.2212841659784317, L1 : 0.036579083651304245\n",
      "epoch 115, loss 0.19106131792068481, L1 : 0.03498263657093048\n",
      "epoch 116, loss 0.19724465906620026, L1 : 0.038515545427799225\n",
      "epoch 117, loss 0.20752470195293427, L1 : 0.04020414501428604\n",
      "epoch 118, loss 0.20341937243938446, L1 : 0.05250100791454315\n",
      "epoch 119, loss 0.23429107666015625, L1 : 0.04100669547915459\n",
      "epoch 120, loss 0.19958484172821045, L1 : 0.04433682933449745\n",
      "epoch 121, loss 0.18546606600284576, L1 : 0.048683397471904755\n",
      "epoch 122, loss 0.19845569133758545, L1 : 0.0410352386534214\n",
      "epoch 123, loss 0.16092872619628906, L1 : 0.03610173612833023\n",
      "epoch 124, loss 0.18306474387645721, L1 : 0.040689386427402496\n",
      "epoch 125, loss 0.18436598777770996, L1 : 0.05072041228413582\n",
      "epoch 126, loss 0.1823870986700058, L1 : 0.038993313908576965\n",
      "epoch 127, loss 0.21012553572654724, L1 : 0.048932116478681564\n",
      "epoch 128, loss 0.18871276080608368, L1 : 0.04804578796029091\n",
      "epoch 129, loss 0.17269766330718994, L1 : 0.03877928853034973\n",
      "epoch 130, loss 0.1699051409959793, L1 : 0.03807146102190018\n",
      "epoch 131, loss 0.19606417417526245, L1 : 0.036252494901418686\n",
      "epoch 132, loss 0.16968144476413727, L1 : 0.043753381818532944\n",
      "epoch 133, loss 0.18293194472789764, L1 : 0.03988105431199074\n",
      "epoch 134, loss 0.18519601225852966, L1 : 0.043053727596998215\n",
      "epoch 135, loss 0.17784790694713593, L1 : 0.03900189697742462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 136, loss 0.15537381172180176, L1 : 0.041822001338005066\n",
      "epoch 137, loss 0.18847277760505676, L1 : 0.03669014573097229\n",
      "epoch 138, loss 0.1719084084033966, L1 : 0.03978312388062477\n",
      "epoch 139, loss 0.17672927677631378, L1 : 0.0429484024643898\n",
      "epoch 140, loss 0.1905364692211151, L1 : 0.044138599187135696\n",
      "epoch 141, loss 0.18946939706802368, L1 : 0.044099386781454086\n",
      "epoch 142, loss 0.17744134366512299, L1 : 0.04297901690006256\n",
      "epoch 143, loss 0.17004844546318054, L1 : 0.0434664785861969\n",
      "epoch 144, loss 0.16876475512981415, L1 : 0.04558437317609787\n",
      "epoch 145, loss 0.18766725063323975, L1 : 0.038686223328113556\n",
      "epoch 146, loss 0.17345505952835083, L1 : 0.03977752476930618\n",
      "epoch 147, loss 0.1777910739183426, L1 : 0.03590425103902817\n",
      "epoch 148, loss 0.14750927686691284, L1 : 0.03582204133272171\n",
      "epoch 149, loss 0.14637112617492676, L1 : 0.044340286403894424\n",
      "epoch 150, loss 0.1590231955051422, L1 : 0.04704568162560463\n",
      "epoch 151, loss 0.15862420201301575, L1 : 0.03906445950269699\n",
      "epoch 152, loss 0.1633128970861435, L1 : 0.03723665699362755\n",
      "epoch 153, loss 0.16944339871406555, L1 : 0.048940468579530716\n",
      "epoch 154, loss 0.14522922039031982, L1 : 0.03852790594100952\n",
      "epoch 155, loss 0.15044726431369781, L1 : 0.04360934719443321\n",
      "epoch 156, loss 0.15005722641944885, L1 : 0.03739180415868759\n",
      "epoch 157, loss 0.1499689221382141, L1 : 0.04632497578859329\n",
      "epoch 158, loss 0.15607279539108276, L1 : 0.04821745678782463\n",
      "epoch 159, loss 0.1458372175693512, L1 : 0.04318837448954582\n",
      "epoch 160, loss 0.1510402262210846, L1 : 0.03550803288817406\n",
      "epoch 161, loss 0.1403554230928421, L1 : 0.04476295784115791\n",
      "epoch 162, loss 0.15137368440628052, L1 : 0.040271446108818054\n",
      "epoch 163, loss 0.14622657001018524, L1 : 0.03435438871383667\n",
      "epoch 164, loss 0.14058414101600647, L1 : 0.035292450338602066\n",
      "epoch 165, loss 0.16776907444000244, L1 : 0.04255075752735138\n",
      "epoch 166, loss 0.14213630557060242, L1 : 0.03675378859043121\n",
      "epoch 167, loss 0.1681828796863556, L1 : 0.04464082792401314\n",
      "epoch 168, loss 0.14624080061912537, L1 : 0.03606150671839714\n",
      "epoch 169, loss 0.168276846408844, L1 : 0.03881564736366272\n",
      "epoch 170, loss 0.15170198678970337, L1 : 0.040659889578819275\n",
      "epoch 171, loss 0.16197481751441956, L1 : 0.041492726653814316\n",
      "epoch 172, loss 0.14601494371891022, L1 : 0.03635227307677269\n",
      "epoch 173, loss 0.14795619249343872, L1 : 0.042493704706430435\n",
      "epoch 174, loss 0.14561516046524048, L1 : 0.043847378343343735\n",
      "epoch 175, loss 0.14262859523296356, L1 : 0.049927033483982086\n",
      "epoch 176, loss 0.14931559562683105, L1 : 0.04026903584599495\n",
      "epoch 177, loss 0.15156957507133484, L1 : 0.04661014303565025\n",
      "epoch 178, loss 0.16566380858421326, L1 : 0.043405186384916306\n",
      "epoch 179, loss 0.16507622599601746, L1 : 0.04605136066675186\n",
      "epoch 180, loss 0.16516023874282837, L1 : 0.04329377040266991\n",
      "epoch 181, loss 0.14939136803150177, L1 : 0.037728194147348404\n",
      "epoch 182, loss 0.1257268190383911, L1 : 0.039984241127967834\n",
      "epoch 183, loss 0.14334088563919067, L1 : 0.03458700701594353\n",
      "epoch 184, loss 0.13861271739006042, L1 : 0.03177007660269737\n",
      "epoch 185, loss 0.13395564258098602, L1 : 0.03459255024790764\n",
      "epoch 186, loss 0.1435864269733429, L1 : 0.040272392332553864\n",
      "epoch 187, loss 0.14781564474105835, L1 : 0.0429631732404232\n",
      "epoch 188, loss 0.13799142837524414, L1 : 0.04153728112578392\n",
      "epoch 189, loss 0.14006364345550537, L1 : 0.0393223837018013\n",
      "epoch 190, loss 0.1355699747800827, L1 : 0.040302835404872894\n",
      "epoch 191, loss 0.14345258474349976, L1 : 0.04243965819478035\n",
      "epoch 192, loss 0.13395194709300995, L1 : 0.04397747293114662\n",
      "epoch 193, loss 0.14449557662010193, L1 : 0.044120319187641144\n",
      "epoch 194, loss 0.13454878330230713, L1 : 0.041820868849754333\n",
      "epoch 195, loss 0.13514816761016846, L1 : 0.035521864891052246\n",
      "epoch 196, loss 0.13310664892196655, L1 : 0.043969329446554184\n",
      "epoch 197, loss 0.11878551542758942, L1 : 0.02842886745929718\n",
      "epoch 198, loss 0.1397915929555893, L1 : 0.04043857753276825\n",
      "epoch 199, loss 0.15356288850307465, L1 : 0.04725613445043564\n",
      "epoch 200, loss 0.14155620336532593, L1 : 0.044808581471443176\n",
      "epoch 201, loss 0.1441057026386261, L1 : 0.038882505148649216\n",
      "epoch 202, loss 0.13052722811698914, L1 : 0.03730488196015358\n",
      "epoch 203, loss 0.15318915247917175, L1 : 0.04160187020897865\n",
      "epoch 204, loss 0.14556889235973358, L1 : 0.03668439760804176\n",
      "epoch 205, loss 0.1292816698551178, L1 : 0.04098079726099968\n",
      "epoch 206, loss 0.13041943311691284, L1 : 0.03562721982598305\n",
      "epoch 207, loss 0.1396723985671997, L1 : 0.04446006566286087\n",
      "epoch 208, loss 0.14188353717327118, L1 : 0.0380857028067112\n",
      "epoch 209, loss 0.139821857213974, L1 : 0.04170117899775505\n",
      "epoch 210, loss 0.13786233961582184, L1 : 0.04367062449455261\n",
      "epoch 211, loss 0.1276829093694687, L1 : 0.040822919458150864\n",
      "epoch 212, loss 0.12894746661186218, L1 : 0.039807531982660294\n",
      "epoch 213, loss 0.13402925431728363, L1 : 0.03948243707418442\n",
      "epoch 214, loss 0.1424015313386917, L1 : 0.040985267609357834\n",
      "epoch 215, loss 0.13169145584106445, L1 : 0.039253633469343185\n",
      "epoch 216, loss 0.1369054764509201, L1 : 0.03851745277643204\n",
      "epoch 217, loss 0.1371355503797531, L1 : 0.03431474417448044\n",
      "epoch 218, loss 0.13852021098136902, L1 : 0.042169373482465744\n",
      "epoch 219, loss 0.1437256783246994, L1 : 0.03934025391936302\n",
      "epoch 220, loss 0.12811362743377686, L1 : 0.04397275671362877\n",
      "epoch 221, loss 0.13266780972480774, L1 : 0.04300329461693764\n",
      "epoch 222, loss 0.12458662688732147, L1 : 0.039774879813194275\n",
      "epoch 223, loss 0.11972682178020477, L1 : 0.039541974663734436\n",
      "epoch 224, loss 0.12863340973854065, L1 : 0.04153101518750191\n",
      "epoch 225, loss 0.13075169920921326, L1 : 0.03505946695804596\n",
      "epoch 226, loss 0.13261589407920837, L1 : 0.03997569531202316\n",
      "epoch 227, loss 0.12526176869869232, L1 : 0.03807003051042557\n",
      "epoch 228, loss 0.1277870237827301, L1 : 0.039523329585790634\n",
      "epoch 229, loss 0.12615397572517395, L1 : 0.042221616953611374\n",
      "epoch 230, loss 0.13601446151733398, L1 : 0.03948976844549179\n",
      "epoch 231, loss 0.13504377007484436, L1 : 0.03644895926117897\n",
      "epoch 232, loss 0.14149203896522522, L1 : 0.04057792201638222\n",
      "epoch 233, loss 0.12439244985580444, L1 : 0.03918293118476868\n",
      "epoch 234, loss 0.12764418125152588, L1 : 0.04549713805317879\n",
      "epoch 235, loss 0.13264545798301697, L1 : 0.040679268538951874\n",
      "epoch 236, loss 0.11775655299425125, L1 : 0.03560271114110947\n",
      "epoch 237, loss 0.12858888506889343, L1 : 0.04428916424512863\n",
      "epoch 238, loss 0.12483745813369751, L1 : 0.04036577418446541\n",
      "epoch 239, loss 0.11792810261249542, L1 : 0.039470743387937546\n",
      "epoch 240, loss 0.13011807203292847, L1 : 0.03933440521359444\n",
      "epoch 241, loss 0.13540542125701904, L1 : 0.041583698242902756\n",
      "epoch 242, loss 0.10417325049638748, L1 : 0.03843905031681061\n",
      "epoch 243, loss 0.1319776475429535, L1 : 0.04370025917887688\n",
      "epoch 244, loss 0.11874847859144211, L1 : 0.03990372642874718\n",
      "epoch 245, loss 0.12214232981204987, L1 : 0.039991460740566254\n",
      "epoch 246, loss 0.11456151306629181, L1 : 0.04045500233769417\n",
      "epoch 247, loss 0.11810106039047241, L1 : 0.041417572647333145\n",
      "epoch 248, loss 0.12905336916446686, L1 : 0.03889663517475128\n",
      "epoch 249, loss 0.09851783514022827, L1 : 0.03381764143705368\n",
      "epoch 250, loss 0.14760884642601013, L1 : 0.040355876088142395\n",
      "epoch 251, loss 0.10801210254430771, L1 : 0.039793867617845535\n",
      "epoch 252, loss 0.12877243757247925, L1 : 0.044896338135004044\n",
      "epoch 253, loss 0.12701281905174255, L1 : 0.04698828235268593\n",
      "epoch 254, loss 0.1195417121052742, L1 : 0.0378943607211113\n",
      "epoch 255, loss 0.12337112426757812, L1 : 0.04241866618394852\n",
      "epoch 256, loss 0.12453697621822357, L1 : 0.034050919115543365\n",
      "epoch 257, loss 0.11938980221748352, L1 : 0.03564343973994255\n",
      "epoch 258, loss 0.12138786166906357, L1 : 0.0393642783164978\n",
      "epoch 259, loss 0.11701278388500214, L1 : 0.03896062448620796\n",
      "epoch 260, loss 0.12737666070461273, L1 : 0.04570946842432022\n",
      "epoch 261, loss 0.12495172768831253, L1 : 0.03626050427556038\n",
      "epoch 262, loss 0.12626522779464722, L1 : 0.0401594378054142\n",
      "epoch 263, loss 0.1264490783214569, L1 : 0.03990642726421356\n",
      "epoch 264, loss 0.09994332492351532, L1 : 0.040258605033159256\n",
      "epoch 265, loss 0.10701940208673477, L1 : 0.038706954568624496\n",
      "epoch 266, loss 0.12273058295249939, L1 : 0.04262247309088707\n",
      "epoch 267, loss 0.11948876827955246, L1 : 0.03689023479819298\n",
      "epoch 268, loss 0.12457051128149033, L1 : 0.042905084788799286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 269, loss 0.11384491622447968, L1 : 0.03864597901701927\n",
      "epoch 270, loss 0.1288173496723175, L1 : 0.03985544666647911\n",
      "epoch 271, loss 0.12046679854393005, L1 : 0.04160219430923462\n",
      "epoch 272, loss 0.10944846272468567, L1 : 0.04568585008382797\n",
      "epoch 273, loss 0.11156962811946869, L1 : 0.03528079017996788\n",
      "epoch 274, loss 0.11534976959228516, L1 : 0.0402890220284462\n",
      "epoch 275, loss 0.12725789844989777, L1 : 0.03655015677213669\n",
      "epoch 276, loss 0.11237985640764236, L1 : 0.04106797277927399\n",
      "epoch 277, loss 0.1303921639919281, L1 : 0.04163757711648941\n",
      "epoch 278, loss 0.11814787983894348, L1 : 0.038603633642196655\n",
      "epoch 279, loss 0.12453020364046097, L1 : 0.038844529539346695\n",
      "epoch 280, loss 0.1360427290201187, L1 : 0.04300164803862572\n",
      "epoch 281, loss 0.12813782691955566, L1 : 0.040048740804195404\n",
      "epoch 282, loss 0.10921104997396469, L1 : 0.038956545293331146\n",
      "epoch 283, loss 0.1033731997013092, L1 : 0.03553619235754013\n",
      "epoch 284, loss 0.12595394253730774, L1 : 0.042982738465070724\n",
      "epoch 285, loss 0.11808338016271591, L1 : 0.03938499838113785\n",
      "epoch 286, loss 0.1085224598646164, L1 : 0.03313629701733589\n",
      "epoch 287, loss 0.11717464029788971, L1 : 0.03903721645474434\n",
      "epoch 288, loss 0.13069412112236023, L1 : 0.040147099643945694\n",
      "epoch 289, loss 0.11341927200555801, L1 : 0.03815319016575813\n",
      "epoch 290, loss 0.11612827330827713, L1 : 0.03905421867966652\n",
      "epoch 291, loss 0.11832751333713531, L1 : 0.038096312433481216\n",
      "epoch 292, loss 0.12132436037063599, L1 : 0.04433572664856911\n",
      "epoch 293, loss 0.1137302964925766, L1 : 0.034751005470752716\n",
      "epoch 294, loss 0.12922760844230652, L1 : 0.041427336633205414\n",
      "epoch 295, loss 0.12490630149841309, L1 : 0.03735104575753212\n",
      "epoch 296, loss 0.10991734266281128, L1 : 0.03893487900495529\n",
      "epoch 297, loss 0.12327100336551666, L1 : 0.04105719178915024\n",
      "epoch 298, loss 0.1148100271821022, L1 : 0.03785889223217964\n",
      "epoch 299, loss 0.11613553762435913, L1 : 0.0348065011203289\n",
      "epoch 300, loss 0.13360337913036346, L1 : 0.043871279805898666\n",
      "epoch 301, loss 0.11379121989011765, L1 : 0.0378904826939106\n",
      "epoch 302, loss 0.11306945979595184, L1 : 0.042161110788583755\n",
      "epoch 303, loss 0.12131717801094055, L1 : 0.03607635572552681\n",
      "epoch 304, loss 0.11760012805461884, L1 : 0.03875330090522766\n",
      "epoch 305, loss 0.11807732284069061, L1 : 0.04257774353027344\n",
      "epoch 306, loss 0.1210964173078537, L1 : 0.041975975036621094\n",
      "epoch 307, loss 0.12806497514247894, L1 : 0.0464642159640789\n",
      "epoch 308, loss 0.11166492104530334, L1 : 0.03654861077666283\n",
      "epoch 309, loss 0.11930794268846512, L1 : 0.04446544498205185\n",
      "epoch 310, loss 0.11100416630506516, L1 : 0.038958095014095306\n",
      "epoch 311, loss 0.12558457255363464, L1 : 0.04121776670217514\n",
      "epoch 312, loss 0.13163480162620544, L1 : 0.03881368413567543\n",
      "epoch 313, loss 0.11732035875320435, L1 : 0.03773893788456917\n",
      "epoch 314, loss 0.12463976442813873, L1 : 0.04412573575973511\n",
      "epoch 315, loss 0.12982821464538574, L1 : 0.04378187656402588\n",
      "epoch 316, loss 0.1181669533252716, L1 : 0.041616134345531464\n",
      "epoch 317, loss 0.1162763312458992, L1 : 0.039563894271850586\n",
      "epoch 318, loss 0.10029500722885132, L1 : 0.03604463115334511\n",
      "epoch 319, loss 0.11071990430355072, L1 : 0.03736986592411995\n",
      "epoch 320, loss 0.12501633167266846, L1 : 0.03581176698207855\n",
      "epoch 321, loss 0.11036299914121628, L1 : 0.041556209325790405\n",
      "epoch 322, loss 0.11554214358329773, L1 : 0.036849357187747955\n",
      "epoch 323, loss 0.1173209473490715, L1 : 0.04235481843352318\n",
      "epoch 324, loss 0.10968318581581116, L1 : 0.041837695986032486\n",
      "epoch 325, loss 0.12649746239185333, L1 : 0.04408969357609749\n",
      "epoch 326, loss 0.12747140228748322, L1 : 0.04342836141586304\n",
      "epoch 327, loss 0.10765016078948975, L1 : 0.038431864231824875\n",
      "epoch 328, loss 0.13478274643421173, L1 : 0.04074377939105034\n",
      "epoch 329, loss 0.10942988097667694, L1 : 0.037653207778930664\n",
      "epoch 330, loss 0.10575270652770996, L1 : 0.03623530641198158\n",
      "epoch 331, loss 0.1279234141111374, L1 : 0.038448266685009\n",
      "epoch 332, loss 0.1214318722486496, L1 : 0.0485517643392086\n",
      "epoch 333, loss 0.10804392397403717, L1 : 0.04355397820472717\n",
      "epoch 334, loss 0.12222679704427719, L1 : 0.03587010130286217\n",
      "epoch 335, loss 0.11440125852823257, L1 : 0.040151990950107574\n",
      "epoch 336, loss 0.12392554432153702, L1 : 0.04801325872540474\n",
      "epoch 337, loss 0.11570262163877487, L1 : 0.03798333555459976\n",
      "epoch 338, loss 0.11888681352138519, L1 : 0.036528218537569046\n",
      "epoch 339, loss 0.1296190321445465, L1 : 0.04103894531726837\n",
      "epoch 340, loss 0.1279047131538391, L1 : 0.04391441121697426\n",
      "epoch 341, loss 0.11444380134344101, L1 : 0.044471241533756256\n",
      "epoch 342, loss 0.1066383421421051, L1 : 0.04086842015385628\n",
      "epoch 343, loss 0.11712422966957092, L1 : 0.04590395838022232\n",
      "epoch 344, loss 0.11001747846603394, L1 : 0.03366608917713165\n",
      "epoch 345, loss 0.11329915374517441, L1 : 0.0421164445579052\n",
      "epoch 346, loss 0.11325563490390778, L1 : 0.04318471997976303\n",
      "epoch 347, loss 0.11358162015676498, L1 : 0.04430365562438965\n",
      "epoch 348, loss 0.11603587120771408, L1 : 0.04493287205696106\n",
      "epoch 349, loss 0.10007607191801071, L1 : 0.03762078285217285\n",
      "epoch 350, loss 0.12347199767827988, L1 : 0.04004552960395813\n",
      "epoch 351, loss 0.1223868802189827, L1 : 0.042461249977350235\n",
      "epoch 352, loss 0.10651230812072754, L1 : 0.040977295488119125\n",
      "epoch 353, loss 0.11087764799594879, L1 : 0.03997137397527695\n",
      "epoch 354, loss 0.12124353647232056, L1 : 0.04397682100534439\n",
      "epoch 355, loss 0.12802447378635406, L1 : 0.04187560826539993\n",
      "epoch 356, loss 0.11349326372146606, L1 : 0.03933697193861008\n",
      "epoch 357, loss 0.10864841938018799, L1 : 0.03932946175336838\n",
      "epoch 358, loss 0.11424793303012848, L1 : 0.040254462510347366\n",
      "epoch 359, loss 0.11303853988647461, L1 : 0.03732016310095787\n",
      "epoch 360, loss 0.11102244257926941, L1 : 0.03586244955658913\n",
      "epoch 361, loss 0.11994452774524689, L1 : 0.04416590556502342\n",
      "epoch 362, loss 0.12347742915153503, L1 : 0.04812563955783844\n",
      "epoch 363, loss 0.11110635101795197, L1 : 0.04159632697701454\n",
      "epoch 364, loss 0.11152123659849167, L1 : 0.039262786507606506\n",
      "epoch 365, loss 0.11397068947553635, L1 : 0.04544166103005409\n",
      "epoch 366, loss 0.1203915923833847, L1 : 0.039079681038856506\n",
      "epoch 367, loss 0.10209324955940247, L1 : 0.03863701596856117\n",
      "epoch 368, loss 0.12032583355903625, L1 : 0.04384812340140343\n",
      "epoch 369, loss 0.10866528004407883, L1 : 0.0376443974673748\n",
      "epoch 370, loss 0.11615528166294098, L1 : 0.04201807826757431\n",
      "epoch 371, loss 0.12447315454483032, L1 : 0.04541764035820961\n",
      "epoch 372, loss 0.11494481563568115, L1 : 0.04253087565302849\n",
      "epoch 373, loss 0.11591392755508423, L1 : 0.039255689829587936\n",
      "epoch 374, loss 0.11136472970247269, L1 : 0.04140937328338623\n",
      "epoch 375, loss 0.12530694901943207, L1 : 0.043251149356365204\n",
      "epoch 376, loss 0.09622110426425934, L1 : 0.04252117872238159\n",
      "epoch 377, loss 0.1253003627061844, L1 : 0.04206691309809685\n",
      "epoch 378, loss 0.11509237438440323, L1 : 0.04378052055835724\n",
      "epoch 379, loss 0.12196558713912964, L1 : 0.03751111030578613\n",
      "epoch 380, loss 0.11529473215341568, L1 : 0.044817086309194565\n",
      "epoch 381, loss 0.11300346255302429, L1 : 0.04118358716368675\n",
      "epoch 382, loss 0.10865872353315353, L1 : 0.04131735861301422\n",
      "epoch 383, loss 0.11184509098529816, L1 : 0.042454320937395096\n",
      "epoch 384, loss 0.1180199459195137, L1 : 0.04304300993680954\n",
      "epoch 385, loss 0.1205696240067482, L1 : 0.03870289772748947\n",
      "epoch 386, loss 0.12394702434539795, L1 : 0.04934999346733093\n",
      "epoch 387, loss 0.12359702587127686, L1 : 0.04619600996375084\n",
      "epoch 388, loss 0.09640182554721832, L1 : 0.033844515681266785\n",
      "epoch 389, loss 0.1121341809630394, L1 : 0.037334371358156204\n",
      "epoch 390, loss 0.11990538239479065, L1 : 0.03843699023127556\n",
      "epoch 391, loss 0.11017951369285583, L1 : 0.03584946691989899\n",
      "epoch 392, loss 0.11787864565849304, L1 : 0.04126131162047386\n",
      "epoch 393, loss 0.11759768426418304, L1 : 0.03885984793305397\n",
      "epoch 394, loss 0.11392081528902054, L1 : 0.03875446319580078\n",
      "epoch 395, loss 0.1133037731051445, L1 : 0.0383746512234211\n",
      "epoch 396, loss 0.11561017483472824, L1 : 0.04390978068113327\n",
      "epoch 397, loss 0.1183762177824974, L1 : 0.037947334349155426\n",
      "epoch 398, loss 0.10962293297052383, L1 : 0.0399310439825058\n",
      "epoch 399, loss 0.09883492439985275, L1 : 0.03812611848115921\n",
      "epoch 400, loss 0.11545335501432419, L1 : 0.043586861342191696\n",
      "epoch 401, loss 0.11648456007242203, L1 : 0.04185426980257034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 402, loss 0.10834585130214691, L1 : 0.04644748196005821\n",
      "epoch 403, loss 0.11464323848485947, L1 : 0.040640540421009064\n",
      "epoch 404, loss 0.10928664356470108, L1 : 0.0443536639213562\n",
      "epoch 405, loss 0.1017591804265976, L1 : 0.03957458212971687\n",
      "epoch 406, loss 0.11796721816062927, L1 : 0.0433506965637207\n",
      "epoch 407, loss 0.11394514888525009, L1 : 0.04177413508296013\n",
      "epoch 408, loss 0.10508839786052704, L1 : 0.04010109230875969\n",
      "epoch 409, loss 0.10301774740219116, L1 : 0.03818459436297417\n",
      "epoch 410, loss 0.11977110058069229, L1 : 0.043117210268974304\n",
      "epoch 411, loss 0.10789680480957031, L1 : 0.045406319200992584\n",
      "epoch 412, loss 0.1142573356628418, L1 : 0.03822159394621849\n",
      "epoch 413, loss 0.11379235237836838, L1 : 0.04430948942899704\n",
      "epoch 414, loss 0.1049923524260521, L1 : 0.034583680331707\n",
      "epoch 415, loss 0.11682863533496857, L1 : 0.03969103842973709\n",
      "epoch 416, loss 0.11799569427967072, L1 : 0.03781607002019882\n",
      "epoch 417, loss 0.09964827448129654, L1 : 0.03807806968688965\n",
      "epoch 418, loss 0.11052499711513519, L1 : 0.04261741414666176\n",
      "epoch 419, loss 0.1053168997168541, L1 : 0.043322931975126266\n",
      "epoch 420, loss 0.11844170093536377, L1 : 0.03313934803009033\n",
      "epoch 421, loss 0.09891491383314133, L1 : 0.036747656762599945\n",
      "epoch 422, loss 0.11084737628698349, L1 : 0.03909803926944733\n",
      "epoch 423, loss 0.10590294003486633, L1 : 0.03734954074025154\n",
      "epoch 424, loss 0.09560902416706085, L1 : 0.04054044187068939\n",
      "epoch 425, loss 0.1109275072813034, L1 : 0.04377484694123268\n",
      "epoch 426, loss 0.11915741860866547, L1 : 0.04304696246981621\n",
      "epoch 427, loss 0.12230721861124039, L1 : 0.04453904554247856\n",
      "epoch 428, loss 0.10536560416221619, L1 : 0.038195669651031494\n",
      "epoch 429, loss 0.10824069380760193, L1 : 0.04219924286007881\n",
      "epoch 430, loss 0.10864794254302979, L1 : 0.042271822690963745\n",
      "epoch 431, loss 0.11219517141580582, L1 : 0.04197857901453972\n",
      "epoch 432, loss 0.10758897662162781, L1 : 0.042472485452890396\n",
      "epoch 433, loss 0.1350482702255249, L1 : 0.045898742973804474\n",
      "epoch 434, loss 0.10388947278261185, L1 : 0.0431484691798687\n",
      "epoch 435, loss 0.11037913709878922, L1 : 0.03939857706427574\n",
      "epoch 436, loss 0.11345222592353821, L1 : 0.041960619390010834\n",
      "epoch 437, loss 0.11582471430301666, L1 : 0.0456576943397522\n",
      "epoch 438, loss 0.10061467438936234, L1 : 0.03709998354315758\n",
      "epoch 439, loss 0.10626965016126633, L1 : 0.03721164911985397\n",
      "epoch 440, loss 0.10905024409294128, L1 : 0.04279546067118645\n",
      "epoch 441, loss 0.10408420115709305, L1 : 0.039097100496292114\n",
      "epoch 442, loss 0.11601332575082779, L1 : 0.032827552407979965\n",
      "epoch 443, loss 0.1011439859867096, L1 : 0.03622995689511299\n",
      "epoch 444, loss 0.11632804572582245, L1 : 0.03824133053421974\n",
      "epoch 445, loss 0.10501211881637573, L1 : 0.040554311126470566\n",
      "epoch 446, loss 0.1004868596792221, L1 : 0.03588862344622612\n",
      "epoch 447, loss 0.11585955321788788, L1 : 0.03636855632066727\n",
      "epoch 448, loss 0.10949722677469254, L1 : 0.04378392919898033\n",
      "epoch 449, loss 0.10300229489803314, L1 : 0.03814743086695671\n",
      "epoch 450, loss 0.11323803663253784, L1 : 0.03761020302772522\n",
      "epoch 451, loss 0.10974887013435364, L1 : 0.04313591495156288\n",
      "epoch 452, loss 0.11125051975250244, L1 : 0.04761282727122307\n",
      "epoch 453, loss 0.11793631315231323, L1 : 0.04065757617354393\n",
      "epoch 454, loss 0.11809475719928741, L1 : 0.04196212440729141\n",
      "epoch 455, loss 0.13346081972122192, L1 : 0.04575381055474281\n",
      "epoch 456, loss 0.11430017650127411, L1 : 0.038478344678878784\n",
      "epoch 457, loss 0.10521779209375381, L1 : 0.03674264997243881\n",
      "epoch 458, loss 0.09309477359056473, L1 : 0.03738563135266304\n",
      "epoch 459, loss 0.11201698333024979, L1 : 0.04205694422125816\n",
      "epoch 460, loss 0.1061871200799942, L1 : 0.04136886075139046\n",
      "epoch 461, loss 0.10926555097103119, L1 : 0.04252028837800026\n",
      "epoch 462, loss 0.10944323986768723, L1 : 0.041588764637708664\n",
      "epoch 463, loss 0.0989745706319809, L1 : 0.03889083117246628\n",
      "epoch 464, loss 0.1110120639204979, L1 : 0.03849836066365242\n",
      "epoch 465, loss 0.10207733511924744, L1 : 0.036959823220968246\n",
      "epoch 466, loss 0.12126956880092621, L1 : 0.04751960188150406\n",
      "epoch 467, loss 0.11918739974498749, L1 : 0.03937938064336777\n",
      "epoch 468, loss 0.10853241384029388, L1 : 0.0400153212249279\n",
      "epoch 469, loss 0.10419457405805588, L1 : 0.03890025615692139\n",
      "epoch 470, loss 0.10731346160173416, L1 : 0.04114691540598869\n",
      "epoch 471, loss 0.11647412925958633, L1 : 0.04357319697737694\n",
      "epoch 472, loss 0.10544276982545853, L1 : 0.039934318512678146\n",
      "epoch 473, loss 0.11992073059082031, L1 : 0.048528265208005905\n",
      "epoch 474, loss 0.1125018373131752, L1 : 0.03995128348469734\n",
      "epoch 475, loss 0.11107068508863449, L1 : 0.03592197597026825\n",
      "epoch 476, loss 0.10199281573295593, L1 : 0.0393165685236454\n",
      "epoch 477, loss 0.10019468516111374, L1 : 0.03649621084332466\n",
      "epoch 478, loss 0.11495944112539291, L1 : 0.04367052763700485\n",
      "epoch 479, loss 0.11186949908733368, L1 : 0.04701634496450424\n",
      "epoch 480, loss 0.10562574863433838, L1 : 0.03998132422566414\n",
      "epoch 481, loss 0.10680001974105835, L1 : 0.04796161875128746\n",
      "epoch 482, loss 0.09793749451637268, L1 : 0.03898511826992035\n",
      "epoch 483, loss 0.10202264785766602, L1 : 0.036680009216070175\n",
      "epoch 484, loss 0.11536505073308945, L1 : 0.04889269545674324\n",
      "epoch 485, loss 0.10742294043302536, L1 : 0.040394753217697144\n",
      "epoch 486, loss 0.09731525182723999, L1 : 0.03699934482574463\n",
      "epoch 487, loss 0.10555881261825562, L1 : 0.036814142018556595\n",
      "epoch 488, loss 0.11029043048620224, L1 : 0.04120463877916336\n",
      "epoch 489, loss 0.11820803582668304, L1 : 0.047820400446653366\n",
      "epoch 490, loss 0.11145511269569397, L1 : 0.04413050785660744\n",
      "epoch 491, loss 0.11886459589004517, L1 : 0.04512155055999756\n",
      "epoch 492, loss 0.11059470474720001, L1 : 0.035509973764419556\n",
      "epoch 493, loss 0.10158573091030121, L1 : 0.041547905653715134\n",
      "epoch 494, loss 0.10019457340240479, L1 : 0.036984171718358994\n",
      "epoch 495, loss 0.11434030532836914, L1 : 0.04142957180738449\n",
      "epoch 496, loss 0.11426706612110138, L1 : 0.04281889647245407\n",
      "epoch 497, loss 0.12045413255691528, L1 : 0.043229326605796814\n",
      "epoch 498, loss 0.12202716618776321, L1 : 0.04750698432326317\n",
      "epoch 499, loss 0.1123436689376831, L1 : 0.04525135084986687\n",
      "epoch 500, loss 0.11469338089227676, L1 : 0.03913768008351326\n",
      "epoch 501, loss 0.09874147176742554, L1 : 0.040822889655828476\n",
      "epoch 502, loss 0.10356076806783676, L1 : 0.03597085177898407\n",
      "epoch 503, loss 0.10896100103855133, L1 : 0.04212739318609238\n",
      "epoch 504, loss 0.10805176198482513, L1 : 0.0452008955180645\n",
      "epoch 505, loss 0.10917484760284424, L1 : 0.039154261350631714\n",
      "epoch 506, loss 0.1020665243268013, L1 : 0.04008500277996063\n",
      "epoch 507, loss 0.10301102697849274, L1 : 0.038647349923849106\n",
      "epoch 508, loss 0.10513367503881454, L1 : 0.041426803916692734\n",
      "epoch 509, loss 0.09342322498559952, L1 : 0.03616258502006531\n",
      "epoch 510, loss 0.10863339900970459, L1 : 0.03997498378157616\n",
      "epoch 511, loss 0.10275709629058838, L1 : 0.0370737724006176\n",
      "epoch 512, loss 0.12984564900398254, L1 : 0.04224707931280136\n",
      "epoch 513, loss 0.1148172914981842, L1 : 0.04413125663995743\n",
      "epoch 514, loss 0.11569605767726898, L1 : 0.04331238567829132\n",
      "epoch 515, loss 0.11602328717708588, L1 : 0.04103051498532295\n",
      "epoch 516, loss 0.11376796662807465, L1 : 0.03746825084090233\n",
      "epoch 517, loss 0.1055934876203537, L1 : 0.03536444529891014\n",
      "epoch 518, loss 0.10002018511295319, L1 : 0.04041603207588196\n",
      "epoch 519, loss 0.10856543481349945, L1 : 0.03566413372755051\n",
      "epoch 520, loss 0.11054037511348724, L1 : 0.03757002204656601\n",
      "epoch 521, loss 0.10856616497039795, L1 : 0.04116335138678551\n",
      "epoch 522, loss 0.10562320798635483, L1 : 0.04271020367741585\n",
      "epoch 523, loss 0.12606781721115112, L1 : 0.040403373539447784\n",
      "epoch 524, loss 0.11872953176498413, L1 : 0.03901265189051628\n",
      "epoch 525, loss 0.11548347026109695, L1 : 0.039371147751808167\n",
      "epoch 526, loss 0.10889261215925217, L1 : 0.041804131120443344\n",
      "epoch 527, loss 0.1141122579574585, L1 : 0.049794986844062805\n",
      "epoch 528, loss 0.11746206879615784, L1 : 0.045364584773778915\n",
      "epoch 529, loss 0.11414756625890732, L1 : 0.04389115050435066\n",
      "epoch 530, loss 0.11133940517902374, L1 : 0.040795665234327316\n",
      "epoch 531, loss 0.11054857075214386, L1 : 0.044017937034368515\n",
      "epoch 532, loss 0.10797193646430969, L1 : 0.037849437445402145\n",
      "epoch 533, loss 0.1086624339222908, L1 : 0.03861698508262634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 534, loss 0.10548698157072067, L1 : 0.04035559296607971\n",
      "epoch 535, loss 0.09253666549921036, L1 : 0.03782081976532936\n",
      "epoch 536, loss 0.11286046355962753, L1 : 0.040523890405893326\n",
      "epoch 537, loss 0.11768227815628052, L1 : 0.041816163808107376\n",
      "epoch 538, loss 0.11071813851594925, L1 : 0.03852265328168869\n",
      "epoch 539, loss 0.10973313450813293, L1 : 0.03978806361556053\n",
      "epoch 540, loss 0.11818664520978928, L1 : 0.0432608425617218\n",
      "epoch 541, loss 0.11002319306135178, L1 : 0.03809985890984535\n",
      "epoch 542, loss 0.09968115389347076, L1 : 0.04086151346564293\n",
      "epoch 543, loss 0.11655101180076599, L1 : 0.0382470041513443\n",
      "epoch 544, loss 0.1180715560913086, L1 : 0.04143537953495979\n",
      "epoch 545, loss 0.10814959555864334, L1 : 0.036543577909469604\n",
      "epoch 546, loss 0.11082404851913452, L1 : 0.03975294530391693\n",
      "epoch 547, loss 0.1092342957854271, L1 : 0.03993705287575722\n",
      "epoch 548, loss 0.10782959312200546, L1 : 0.044530902057886124\n",
      "epoch 549, loss 0.10590334981679916, L1 : 0.043766748160123825\n",
      "epoch 550, loss 0.1018054410815239, L1 : 0.04170385003089905\n",
      "epoch 551, loss 0.10447438061237335, L1 : 0.043011557310819626\n",
      "epoch 552, loss 0.10266231745481491, L1 : 0.04089895263314247\n",
      "epoch 553, loss 0.11552883684635162, L1 : 0.04320517182350159\n",
      "epoch 554, loss 0.11272180080413818, L1 : 0.04159441217780113\n",
      "epoch 555, loss 0.10991829633712769, L1 : 0.04910263046622276\n",
      "epoch 556, loss 0.10301893204450607, L1 : 0.038755934685468674\n",
      "epoch 557, loss 0.12453167885541916, L1 : 0.04330180585384369\n",
      "epoch 558, loss 0.10638291388750076, L1 : 0.039159663021564484\n",
      "epoch 559, loss 0.10729948431253433, L1 : 0.04358241707086563\n",
      "epoch 560, loss 0.10967222601175308, L1 : 0.041075024753808975\n",
      "epoch 561, loss 0.11017711460590363, L1 : 0.039470069110393524\n",
      "epoch 562, loss 0.11016853153705597, L1 : 0.03402867168188095\n",
      "epoch 563, loss 0.11980520188808441, L1 : 0.03872237354516983\n",
      "epoch 564, loss 0.10021703690290451, L1 : 0.03853970393538475\n",
      "epoch 565, loss 0.10784151405096054, L1 : 0.04253191500902176\n",
      "epoch 566, loss 0.11489031463861465, L1 : 0.04213930293917656\n",
      "epoch 567, loss 0.10980953276157379, L1 : 0.03608359023928642\n",
      "epoch 568, loss 0.116318479180336, L1 : 0.042094334959983826\n",
      "epoch 569, loss 0.1046764999628067, L1 : 0.04388146847486496\n",
      "epoch 570, loss 0.09745664149522781, L1 : 0.039223771542310715\n",
      "epoch 571, loss 0.10804681479930878, L1 : 0.04463080316781998\n",
      "epoch 572, loss 0.11320855468511581, L1 : 0.04488026723265648\n",
      "epoch 573, loss 0.10528004169464111, L1 : 0.03623100742697716\n",
      "epoch 574, loss 0.12231257557868958, L1 : 0.04755224287509918\n",
      "epoch 575, loss 0.10454757511615753, L1 : 0.04066857695579529\n",
      "epoch 576, loss 0.1025984063744545, L1 : 0.04062759503722191\n",
      "epoch 577, loss 0.11927241086959839, L1 : 0.044045377522706985\n",
      "epoch 578, loss 0.10685204714536667, L1 : 0.036676786839962006\n",
      "epoch 579, loss 0.10324473679065704, L1 : 0.040148407220840454\n",
      "epoch 580, loss 0.11327672749757767, L1 : 0.04356785863637924\n",
      "epoch 581, loss 0.10562823712825775, L1 : 0.03977375850081444\n",
      "epoch 582, loss 0.09360480308532715, L1 : 0.03914470225572586\n",
      "epoch 583, loss 0.11113636940717697, L1 : 0.041983526200056076\n",
      "epoch 584, loss 0.09924697875976562, L1 : 0.03588046133518219\n",
      "epoch 585, loss 0.10755987465381622, L1 : 0.035342149436473846\n",
      "epoch 586, loss 0.10621435940265656, L1 : 0.035635530948638916\n",
      "epoch 587, loss 0.1224730908870697, L1 : 0.03997011110186577\n",
      "epoch 588, loss 0.11149392277002335, L1 : 0.04129617288708687\n",
      "epoch 589, loss 0.11891300231218338, L1 : 0.039340607821941376\n",
      "epoch 590, loss 0.09814105182886124, L1 : 0.036677323281764984\n",
      "epoch 591, loss 0.10457077622413635, L1 : 0.03781021013855934\n",
      "epoch 592, loss 0.10274749994277954, L1 : 0.04064297303557396\n",
      "epoch 593, loss 0.1055641621351242, L1 : 0.03663049638271332\n",
      "epoch 594, loss 0.10176600515842438, L1 : 0.03885599225759506\n",
      "epoch 595, loss 0.09066708385944366, L1 : 0.040253810584545135\n",
      "epoch 596, loss 0.11544720828533173, L1 : 0.04511098936200142\n",
      "epoch 597, loss 0.09111198037862778, L1 : 0.033910129219293594\n",
      "epoch 598, loss 0.0988517552614212, L1 : 0.03706447780132294\n",
      "epoch 599, loss 0.11682938039302826, L1 : 0.03798631951212883\n",
      "epoch 600, loss 0.11254025250673294, L1 : 0.046087246388196945\n",
      "epoch 601, loss 0.09905294328927994, L1 : 0.03594746068120003\n",
      "epoch 602, loss 0.11758644133806229, L1 : 0.043109774589538574\n",
      "epoch 603, loss 0.10249628871679306, L1 : 0.04281481355428696\n",
      "epoch 604, loss 0.10986816138029099, L1 : 0.04343235865235329\n",
      "epoch 605, loss 0.10234994441270828, L1 : 0.03863327205181122\n",
      "epoch 606, loss 0.1069321408867836, L1 : 0.04320953041315079\n",
      "epoch 607, loss 0.12450827658176422, L1 : 0.04342152550816536\n",
      "epoch 608, loss 0.10187505185604095, L1 : 0.03820020705461502\n",
      "epoch 609, loss 0.11957832425832748, L1 : 0.04567158594727516\n",
      "epoch 610, loss 0.12502579391002655, L1 : 0.05347154662013054\n",
      "epoch 611, loss 0.10397087782621384, L1 : 0.036785829812288284\n",
      "epoch 612, loss 0.10809636861085892, L1 : 0.03760217875242233\n",
      "epoch 613, loss 0.11278058588504791, L1 : 0.04055122658610344\n",
      "epoch 614, loss 0.11394894868135452, L1 : 0.04508097097277641\n",
      "epoch 615, loss 0.10894230008125305, L1 : 0.03470177203416824\n",
      "epoch 616, loss 0.1083020269870758, L1 : 0.041242774575948715\n",
      "epoch 617, loss 0.1111089214682579, L1 : 0.042332060635089874\n",
      "epoch 618, loss 0.11754626035690308, L1 : 0.04341701790690422\n",
      "epoch 619, loss 0.11080119013786316, L1 : 0.042832016944885254\n",
      "epoch 620, loss 0.10674197971820831, L1 : 0.03728026524186134\n",
      "epoch 621, loss 0.10448723286390305, L1 : 0.039329055696725845\n",
      "epoch 622, loss 0.10701949149370193, L1 : 0.0403028205037117\n",
      "epoch 623, loss 0.0938325971364975, L1 : 0.03868214786052704\n",
      "epoch 624, loss 0.10736691206693649, L1 : 0.039111413061618805\n",
      "epoch 625, loss 0.11091282218694687, L1 : 0.040178731083869934\n",
      "epoch 626, loss 0.1135517805814743, L1 : 0.04178259149193764\n",
      "epoch 627, loss 0.09919006377458572, L1 : 0.040614187717437744\n",
      "epoch 628, loss 0.1026618480682373, L1 : 0.04146721959114075\n",
      "epoch 629, loss 0.11585497856140137, L1 : 0.0464419387280941\n",
      "epoch 630, loss 0.11143530160188675, L1 : 0.042293667793273926\n",
      "epoch 631, loss 0.12121303379535675, L1 : 0.043376289308071136\n",
      "epoch 632, loss 0.1024642214179039, L1 : 0.04307256266474724\n",
      "epoch 633, loss 0.11776670813560486, L1 : 0.04516935721039772\n",
      "epoch 634, loss 0.10838422179222107, L1 : 0.03715038299560547\n",
      "epoch 635, loss 0.11665479093790054, L1 : 0.0419122576713562\n",
      "epoch 636, loss 0.10622131079435349, L1 : 0.03854934871196747\n",
      "epoch 637, loss 0.10881148278713226, L1 : 0.04549877345561981\n",
      "epoch 638, loss 0.1236419826745987, L1 : 0.040524739772081375\n",
      "epoch 639, loss 0.10858386754989624, L1 : 0.04003153741359711\n",
      "epoch 640, loss 0.11094978451728821, L1 : 0.03880443796515465\n",
      "epoch 641, loss 0.10755902528762817, L1 : 0.04402557387948036\n",
      "epoch 642, loss 0.10163376480340958, L1 : 0.042069047689437866\n",
      "epoch 643, loss 0.10122562944889069, L1 : 0.03145870566368103\n",
      "epoch 644, loss 0.10519091039896011, L1 : 0.04292209446430206\n",
      "epoch 645, loss 0.10586896538734436, L1 : 0.039776142686605453\n",
      "epoch 646, loss 0.10306216776371002, L1 : 0.0409218892455101\n",
      "epoch 647, loss 0.1169123649597168, L1 : 0.03756514936685562\n",
      "epoch 648, loss 0.11221708357334137, L1 : 0.04165639728307724\n",
      "epoch 649, loss 0.1118544489145279, L1 : 0.04028914496302605\n",
      "epoch 650, loss 0.10124339908361435, L1 : 0.04007858410477638\n",
      "epoch 651, loss 0.11015878617763519, L1 : 0.0395231656730175\n",
      "epoch 652, loss 0.11501871794462204, L1 : 0.037925951182842255\n",
      "epoch 653, loss 0.10851062089204788, L1 : 0.04454721510410309\n",
      "epoch 654, loss 0.10919727385044098, L1 : 0.04179039224982262\n",
      "epoch 655, loss 0.10851670801639557, L1 : 0.04147849977016449\n",
      "epoch 656, loss 0.10672981292009354, L1 : 0.04296652227640152\n",
      "epoch 657, loss 0.1140572726726532, L1 : 0.03892427682876587\n",
      "epoch 658, loss 0.11984021216630936, L1 : 0.04107845202088356\n",
      "epoch 659, loss 0.10762104392051697, L1 : 0.04092405363917351\n",
      "epoch 660, loss 0.11121142655611038, L1 : 0.041561320424079895\n",
      "epoch 661, loss 0.10435683280229568, L1 : 0.041028209030628204\n",
      "epoch 662, loss 0.10774202644824982, L1 : 0.03828878328204155\n",
      "epoch 663, loss 0.1105411946773529, L1 : 0.033852577209472656\n",
      "epoch 664, loss 0.10810042917728424, L1 : 0.04180828481912613\n",
      "epoch 665, loss 0.1132025495171547, L1 : 0.042483434081077576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 666, loss 0.10217152535915375, L1 : 0.03666767477989197\n",
      "epoch 667, loss 0.1118955984711647, L1 : 0.043215952813625336\n",
      "epoch 668, loss 0.12316031754016876, L1 : 0.045393820852041245\n",
      "epoch 669, loss 0.1080416589975357, L1 : 0.03777685761451721\n",
      "epoch 670, loss 0.1157013550400734, L1 : 0.03969554603099823\n",
      "epoch 671, loss 0.11800475418567657, L1 : 0.0438385009765625\n",
      "epoch 672, loss 0.12274859845638275, L1 : 0.04138394445180893\n",
      "epoch 673, loss 0.09798066318035126, L1 : 0.033840153366327286\n",
      "epoch 674, loss 0.11543938517570496, L1 : 0.042530357837677\n",
      "epoch 675, loss 0.1080549955368042, L1 : 0.04009093716740608\n",
      "epoch 676, loss 0.11467138677835464, L1 : 0.04219095781445503\n",
      "epoch 677, loss 0.10601630806922913, L1 : 0.0393136590719223\n",
      "epoch 678, loss 0.10633032023906708, L1 : 0.03941129520535469\n",
      "epoch 679, loss 0.11526603996753693, L1 : 0.04270939901471138\n",
      "epoch 680, loss 0.1085158959031105, L1 : 0.040919069200754166\n",
      "epoch 681, loss 0.11737793684005737, L1 : 0.04587242007255554\n",
      "epoch 682, loss 0.10571008920669556, L1 : 0.04066593199968338\n",
      "epoch 683, loss 0.10953603684902191, L1 : 0.04035693407058716\n",
      "epoch 684, loss 0.11314824968576431, L1 : 0.04345952346920967\n",
      "epoch 685, loss 0.12742996215820312, L1 : 0.045612137764692307\n",
      "epoch 686, loss 0.10326135903596878, L1 : 0.04239610955119133\n",
      "epoch 687, loss 0.1112130805850029, L1 : 0.04048505425453186\n",
      "epoch 688, loss 0.10193445533514023, L1 : 0.04040323942899704\n",
      "epoch 689, loss 0.1023128405213356, L1 : 0.03917499631643295\n",
      "epoch 690, loss 0.11110390722751617, L1 : 0.04976538196206093\n",
      "epoch 691, loss 0.1092454269528389, L1 : 0.040026117116212845\n",
      "epoch 692, loss 0.10586107522249222, L1 : 0.04271351546049118\n",
      "epoch 693, loss 0.1178215816617012, L1 : 0.042535580694675446\n",
      "epoch 694, loss 0.10869525372982025, L1 : 0.039430391043424606\n",
      "epoch 695, loss 0.11685983836650848, L1 : 0.040577858686447144\n",
      "epoch 696, loss 0.10157935321331024, L1 : 0.03656788915395737\n",
      "epoch 697, loss 0.10310740023851395, L1 : 0.04260103031992912\n",
      "epoch 698, loss 0.10738303512334824, L1 : 0.040909938514232635\n",
      "epoch 699, loss 0.11779101192951202, L1 : 0.04304921254515648\n",
      "epoch 700, loss 0.11583256721496582, L1 : 0.04191819950938225\n",
      "epoch 701, loss 0.1262933760881424, L1 : 0.038109585642814636\n",
      "epoch 702, loss 0.09839798510074615, L1 : 0.03742207959294319\n",
      "epoch 703, loss 0.09441659599542618, L1 : 0.035098735243082047\n",
      "epoch 704, loss 0.11657504737377167, L1 : 0.042176779359579086\n",
      "epoch 705, loss 0.10578402131795883, L1 : 0.036571938544511795\n",
      "epoch 706, loss 0.10801584273576736, L1 : 0.04117925092577934\n",
      "epoch 707, loss 0.09761592000722885, L1 : 0.04148456081748009\n",
      "epoch 708, loss 0.1087585836648941, L1 : 0.04456930235028267\n",
      "epoch 709, loss 0.11140277236700058, L1 : 0.0446297824382782\n",
      "epoch 710, loss 0.11011594533920288, L1 : 0.04156254231929779\n",
      "epoch 711, loss 0.11203726381063461, L1 : 0.045376360416412354\n",
      "epoch 712, loss 0.11462633311748505, L1 : 0.040736664086580276\n",
      "epoch 713, loss 0.10638260841369629, L1 : 0.044683683663606644\n",
      "epoch 714, loss 0.12136329710483551, L1 : 0.044507335871458054\n",
      "epoch 715, loss 0.11324582993984222, L1 : 0.04506932199001312\n",
      "epoch 716, loss 0.10662086308002472, L1 : 0.0379115492105484\n",
      "epoch 717, loss 0.1165914386510849, L1 : 0.03944513201713562\n",
      "epoch 718, loss 0.10797993093729019, L1 : 0.0395163930952549\n",
      "epoch 719, loss 0.12209335714578629, L1 : 0.04252033680677414\n",
      "epoch 720, loss 0.0982859656214714, L1 : 0.03656165301799774\n",
      "epoch 721, loss 0.11379814147949219, L1 : 0.03854905068874359\n",
      "epoch 722, loss 0.10106911510229111, L1 : 0.04191600903868675\n",
      "epoch 723, loss 0.10943572968244553, L1 : 0.04343634471297264\n",
      "epoch 724, loss 0.09422965347766876, L1 : 0.032644011080265045\n",
      "epoch 725, loss 0.10077783465385437, L1 : 0.03864845260977745\n",
      "epoch 726, loss 0.08864931762218475, L1 : 0.03916729614138603\n",
      "epoch 727, loss 0.11227981746196747, L1 : 0.04265909269452095\n",
      "epoch 728, loss 0.10256931185722351, L1 : 0.03910239413380623\n",
      "epoch 729, loss 0.11400874704122543, L1 : 0.040023211389780045\n",
      "epoch 730, loss 0.11550450325012207, L1 : 0.04774464666843414\n",
      "epoch 731, loss 0.10295699536800385, L1 : 0.03956005722284317\n",
      "epoch 732, loss 0.12412674725055695, L1 : 0.040174562484025955\n",
      "epoch 733, loss 0.11116425693035126, L1 : 0.03781838342547417\n",
      "epoch 734, loss 0.1073586493730545, L1 : 0.042914681136608124\n",
      "epoch 735, loss 0.11043764650821686, L1 : 0.04956367239356041\n",
      "epoch 736, loss 0.10138335824012756, L1 : 0.04012566804885864\n",
      "epoch 737, loss 0.11516587436199188, L1 : 0.03704553097486496\n",
      "epoch 738, loss 0.11201994121074677, L1 : 0.04590953513979912\n",
      "epoch 739, loss 0.09596511721611023, L1 : 0.03774482384324074\n",
      "epoch 740, loss 0.10859955102205276, L1 : 0.03718898072838783\n",
      "epoch 741, loss 0.10690753906965256, L1 : 0.04478420317173004\n",
      "epoch 742, loss 0.10167832672595978, L1 : 0.03393617272377014\n",
      "epoch 743, loss 0.10625186562538147, L1 : 0.0396522581577301\n",
      "epoch 744, loss 0.10758715867996216, L1 : 0.041501689702272415\n",
      "epoch 745, loss 0.12167271971702576, L1 : 0.0439574308693409\n",
      "epoch 746, loss 0.11136447638273239, L1 : 0.03920814022421837\n",
      "epoch 747, loss 0.1120993122458458, L1 : 0.03933132812380791\n",
      "epoch 748, loss 0.10567885637283325, L1 : 0.03809007629752159\n",
      "epoch 749, loss 0.10423921048641205, L1 : 0.037431687116622925\n",
      "epoch 750, loss 0.11166831851005554, L1 : 0.04237724840641022\n",
      "epoch 751, loss 0.11572040617465973, L1 : 0.04478242248296738\n",
      "epoch 752, loss 0.10920948535203934, L1 : 0.04235030710697174\n",
      "epoch 753, loss 0.10264173895120621, L1 : 0.043386198580265045\n",
      "epoch 754, loss 0.10571743547916412, L1 : 0.03913751244544983\n",
      "epoch 755, loss 0.1129642128944397, L1 : 0.045186951756477356\n",
      "epoch 756, loss 0.10726933181285858, L1 : 0.038987964391708374\n",
      "epoch 757, loss 0.12761417031288147, L1 : 0.04975993558764458\n",
      "epoch 758, loss 0.11431606113910675, L1 : 0.03810885176062584\n",
      "epoch 759, loss 0.10337844491004944, L1 : 0.04011058434844017\n",
      "epoch 760, loss 0.10989827662706375, L1 : 0.03897957131266594\n",
      "epoch 761, loss 0.10331270098686218, L1 : 0.04006042331457138\n",
      "epoch 762, loss 0.10725285857915878, L1 : 0.037731483578681946\n",
      "epoch 763, loss 0.11550334095954895, L1 : 0.03930400684475899\n",
      "epoch 764, loss 0.10380005091428757, L1 : 0.038132499903440475\n",
      "epoch 765, loss 0.11658153682947159, L1 : 0.04269789159297943\n",
      "epoch 766, loss 0.10862956196069717, L1 : 0.037996724247932434\n",
      "epoch 767, loss 0.1090550571680069, L1 : 0.041978977620601654\n",
      "epoch 768, loss 0.11821813881397247, L1 : 0.042204272001981735\n",
      "epoch 769, loss 0.09949155896902084, L1 : 0.04124617204070091\n",
      "epoch 770, loss 0.10740181058645248, L1 : 0.04079475998878479\n",
      "epoch 771, loss 0.1071508377790451, L1 : 0.04715990275144577\n",
      "epoch 772, loss 0.09878205507993698, L1 : 0.03642399236559868\n",
      "epoch 773, loss 0.11125397682189941, L1 : 0.04104725271463394\n",
      "epoch 774, loss 0.10357727855443954, L1 : 0.04008514806628227\n",
      "epoch 775, loss 0.10677724331617355, L1 : 0.04615427553653717\n",
      "epoch 776, loss 0.09910471737384796, L1 : 0.03360491245985031\n",
      "epoch 777, loss 0.10806021839380264, L1 : 0.0452498160302639\n",
      "epoch 778, loss 0.11742988228797913, L1 : 0.03865285962820053\n",
      "epoch 779, loss 0.10839086771011353, L1 : 0.03726716712117195\n",
      "epoch 780, loss 0.11032752692699432, L1 : 0.04043757915496826\n",
      "epoch 781, loss 0.11251933127641678, L1 : 0.040305204689502716\n",
      "epoch 782, loss 0.11360418796539307, L1 : 0.03887816518545151\n",
      "epoch 783, loss 0.11638320982456207, L1 : 0.0422801710665226\n",
      "epoch 784, loss 0.10098066926002502, L1 : 0.03872175142168999\n",
      "epoch 785, loss 0.10647106915712357, L1 : 0.04245302453637123\n",
      "epoch 786, loss 0.10563881695270538, L1 : 0.040357884019613266\n",
      "epoch 787, loss 0.11814950406551361, L1 : 0.04808852821588516\n",
      "epoch 788, loss 0.09649357199668884, L1 : 0.038623541593551636\n",
      "epoch 789, loss 0.1073170006275177, L1 : 0.03928281366825104\n",
      "epoch 790, loss 0.12267768383026123, L1 : 0.0388670451939106\n",
      "epoch 791, loss 0.10029823333024979, L1 : 0.036560382694005966\n",
      "epoch 792, loss 0.10632459819316864, L1 : 0.03961968049407005\n",
      "epoch 793, loss 0.10830477625131607, L1 : 0.03664647415280342\n",
      "epoch 794, loss 0.1189693808555603, L1 : 0.043484363704919815\n",
      "epoch 795, loss 0.09899565577507019, L1 : 0.03722888231277466\n",
      "epoch 796, loss 0.09788541495800018, L1 : 0.04327373579144478\n",
      "epoch 797, loss 0.09579836577177048, L1 : 0.03925076872110367\n",
      "epoch 798, loss 0.116716668009758, L1 : 0.03757644444704056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 799, loss 0.12591156363487244, L1 : 0.04279294237494469\n",
      "epoch 800, loss 0.09893915057182312, L1 : 0.03258733078837395\n",
      "epoch 801, loss 0.10843025147914886, L1 : 0.04294093698263168\n",
      "epoch 802, loss 0.11176396906375885, L1 : 0.04141777381300926\n",
      "epoch 803, loss 0.09863518178462982, L1 : 0.035752907395362854\n",
      "epoch 804, loss 0.11901362240314484, L1 : 0.04101540893316269\n",
      "epoch 805, loss 0.10946939885616302, L1 : 0.043755125254392624\n",
      "epoch 806, loss 0.09729593247175217, L1 : 0.037458620965480804\n",
      "epoch 807, loss 0.10401946306228638, L1 : 0.043266598135232925\n",
      "epoch 808, loss 0.10122592747211456, L1 : 0.0405963733792305\n",
      "epoch 809, loss 0.11365154385566711, L1 : 0.03979547694325447\n",
      "epoch 810, loss 0.11340970546007156, L1 : 0.044300589710474014\n",
      "epoch 811, loss 0.12084729969501495, L1 : 0.04850692301988602\n",
      "epoch 812, loss 0.10661091655492783, L1 : 0.0432865247130394\n",
      "epoch 813, loss 0.09974991530179977, L1 : 0.04099353030323982\n",
      "epoch 814, loss 0.10790377110242844, L1 : 0.047545187175273895\n",
      "epoch 815, loss 0.10019046068191528, L1 : 0.036937858909368515\n",
      "epoch 816, loss 0.10231603682041168, L1 : 0.03477437049150467\n",
      "epoch 817, loss 0.12414287030696869, L1 : 0.049000825732946396\n",
      "epoch 818, loss 0.11964568495750427, L1 : 0.04337647929787636\n",
      "epoch 819, loss 0.10826506465673447, L1 : 0.04302582889795303\n",
      "epoch 820, loss 0.10864274203777313, L1 : 0.042738351970911026\n",
      "epoch 821, loss 0.11489824950695038, L1 : 0.043016765266656876\n",
      "epoch 822, loss 0.08475364744663239, L1 : 0.029735902324318886\n",
      "epoch 823, loss 0.1050172969698906, L1 : 0.037527382373809814\n",
      "epoch 824, loss 0.10577186942100525, L1 : 0.037538640201091766\n",
      "epoch 825, loss 0.09974372386932373, L1 : 0.03606699779629707\n",
      "epoch 826, loss 0.10570406913757324, L1 : 0.04170864075422287\n",
      "epoch 827, loss 0.1045847088098526, L1 : 0.037385690957307816\n",
      "epoch 828, loss 0.10962897539138794, L1 : 0.04127544164657593\n",
      "epoch 829, loss 0.09568694978952408, L1 : 0.0417633056640625\n",
      "epoch 830, loss 0.10673381388187408, L1 : 0.03961481899023056\n",
      "epoch 831, loss 0.09908454865217209, L1 : 0.037444036453962326\n",
      "epoch 832, loss 0.09721402823925018, L1 : 0.036780837923288345\n",
      "epoch 833, loss 0.11640828847885132, L1 : 0.04265264794230461\n",
      "epoch 834, loss 0.10404803603887558, L1 : 0.035050202161073685\n",
      "epoch 835, loss 0.10003485530614853, L1 : 0.03516132012009621\n",
      "epoch 836, loss 0.10071151703596115, L1 : 0.0365760400891304\n",
      "epoch 837, loss 0.11347313225269318, L1 : 0.042834408581256866\n",
      "epoch 838, loss 0.12417657673358917, L1 : 0.0376746729016304\n",
      "epoch 839, loss 0.10328540951013565, L1 : 0.043615102767944336\n",
      "epoch 840, loss 0.0988752692937851, L1 : 0.035707589238882065\n",
      "epoch 841, loss 0.11621890962123871, L1 : 0.04610485956072807\n",
      "epoch 842, loss 0.10584025084972382, L1 : 0.03795358166098595\n",
      "epoch 843, loss 0.10833858698606491, L1 : 0.0473499596118927\n",
      "epoch 844, loss 0.10926275700330734, L1 : 0.04042632132768631\n",
      "epoch 845, loss 0.1096026673913002, L1 : 0.04137260466814041\n",
      "epoch 846, loss 0.10271156579256058, L1 : 0.04239058494567871\n",
      "epoch 847, loss 0.10182823240756989, L1 : 0.04282054677605629\n",
      "epoch 848, loss 0.10623299330472946, L1 : 0.043179139494895935\n",
      "epoch 849, loss 0.09920229017734528, L1 : 0.03587467968463898\n",
      "epoch 850, loss 0.10192858427762985, L1 : 0.04239598289132118\n",
      "epoch 851, loss 0.11430181562900543, L1 : 0.036912702023983\n",
      "epoch 852, loss 0.11272954195737839, L1 : 0.041668601334095\n",
      "epoch 853, loss 0.10813730210065842, L1 : 0.03751247748732567\n",
      "epoch 854, loss 0.11389806121587753, L1 : 0.04288448393344879\n",
      "epoch 855, loss 0.09705936908721924, L1 : 0.03832100331783295\n",
      "epoch 856, loss 0.10396342724561691, L1 : 0.041272591799497604\n",
      "epoch 857, loss 0.11697547882795334, L1 : 0.03867514804005623\n",
      "epoch 858, loss 0.11073724925518036, L1 : 0.04398636892437935\n",
      "epoch 859, loss 0.09375545382499695, L1 : 0.04193894565105438\n",
      "epoch 860, loss 0.10390812903642654, L1 : 0.03544545918703079\n",
      "epoch 861, loss 0.11245741695165634, L1 : 0.048439960926771164\n",
      "epoch 862, loss 0.11250675469636917, L1 : 0.04434499517083168\n",
      "epoch 863, loss 0.10398290306329727, L1 : 0.03689273074269295\n",
      "epoch 864, loss 0.11177092045545578, L1 : 0.04056232422590256\n",
      "epoch 865, loss 0.1101219430565834, L1 : 0.04389301314949989\n",
      "epoch 866, loss 0.11881029605865479, L1 : 0.04574960470199585\n",
      "epoch 867, loss 0.10848486423492432, L1 : 0.039017099887132645\n",
      "epoch 868, loss 0.10472767800092697, L1 : 0.04193507134914398\n",
      "epoch 869, loss 0.10178381204605103, L1 : 0.04105375334620476\n",
      "epoch 870, loss 0.10209847241640091, L1 : 0.03669387474656105\n",
      "epoch 871, loss 0.10666760802268982, L1 : 0.04094460606575012\n",
      "epoch 872, loss 0.11056717485189438, L1 : 0.03949085623025894\n",
      "epoch 873, loss 0.10349389910697937, L1 : 0.03970398008823395\n",
      "epoch 874, loss 0.10918325185775757, L1 : 0.03786901384592056\n",
      "epoch 875, loss 0.1228065937757492, L1 : 0.040140550583601\n",
      "epoch 876, loss 0.11101092398166656, L1 : 0.04569542780518532\n",
      "epoch 877, loss 0.11072895675897598, L1 : 0.041413914412260056\n",
      "epoch 878, loss 0.11045020818710327, L1 : 0.039146266877651215\n",
      "epoch 879, loss 0.11095328629016876, L1 : 0.038020774722099304\n",
      "epoch 880, loss 0.11841642111539841, L1 : 0.04629683494567871\n",
      "epoch 881, loss 0.10355177521705627, L1 : 0.03851030394434929\n",
      "epoch 882, loss 0.10921461880207062, L1 : 0.03808475658297539\n",
      "epoch 883, loss 0.11273504793643951, L1 : 0.044680334627628326\n",
      "epoch 884, loss 0.10610778629779816, L1 : 0.04160735011100769\n",
      "epoch 885, loss 0.10613132268190384, L1 : 0.043411701917648315\n",
      "epoch 886, loss 0.11051344871520996, L1 : 0.03559742122888565\n",
      "epoch 887, loss 0.11045108735561371, L1 : 0.039736613631248474\n",
      "epoch 888, loss 0.11690956354141235, L1 : 0.03762569651007652\n",
      "epoch 889, loss 0.1113964393734932, L1 : 0.04144107177853584\n",
      "epoch 890, loss 0.10332752764225006, L1 : 0.04481283947825432\n",
      "epoch 891, loss 0.10680695623159409, L1 : 0.041671015322208405\n",
      "epoch 892, loss 0.09832543134689331, L1 : 0.03428899124264717\n",
      "epoch 893, loss 0.11194340884685516, L1 : 0.042189233005046844\n",
      "epoch 894, loss 0.11646503955125809, L1 : 0.051635224372148514\n",
      "epoch 895, loss 0.09883815050125122, L1 : 0.035753753036260605\n",
      "epoch 896, loss 0.10816804319620132, L1 : 0.0372132882475853\n",
      "epoch 897, loss 0.10026679188013077, L1 : 0.037168391048908234\n",
      "epoch 898, loss 0.11219163239002228, L1 : 0.04466741904616356\n",
      "epoch 899, loss 0.10719679296016693, L1 : 0.039519552141427994\n",
      "epoch 900, loss 0.10598130524158478, L1 : 0.039011985063552856\n",
      "epoch 901, loss 0.11987582594156265, L1 : 0.03922029957175255\n",
      "epoch 902, loss 0.11588436365127563, L1 : 0.04543541371822357\n",
      "epoch 903, loss 0.11376914381980896, L1 : 0.046081509441137314\n",
      "epoch 904, loss 0.1051434725522995, L1 : 0.038995955139398575\n",
      "epoch 905, loss 0.11307043582201004, L1 : 0.040791772305965424\n",
      "epoch 906, loss 0.1133071631193161, L1 : 0.043419960886240005\n",
      "epoch 907, loss 0.11172568798065186, L1 : 0.03953829035162926\n",
      "epoch 908, loss 0.1001942828297615, L1 : 0.036092087626457214\n",
      "epoch 909, loss 0.09914885461330414, L1 : 0.035676274448633194\n",
      "epoch 910, loss 0.11159738898277283, L1 : 0.038800906389951706\n",
      "epoch 911, loss 0.10682913661003113, L1 : 0.03820032626390457\n",
      "epoch 912, loss 0.09639900922775269, L1 : 0.03856709226965904\n",
      "epoch 913, loss 0.10740550607442856, L1 : 0.038822103291749954\n",
      "epoch 914, loss 0.10661378502845764, L1 : 0.04158186540007591\n",
      "epoch 915, loss 0.09323669970035553, L1 : 0.039809029549360275\n",
      "epoch 916, loss 0.10931194573640823, L1 : 0.044389475136995316\n",
      "epoch 917, loss 0.10388027131557465, L1 : 0.03957168012857437\n",
      "epoch 918, loss 0.10746228694915771, L1 : 0.045206841081380844\n",
      "epoch 919, loss 0.10814128816127777, L1 : 0.04016635939478874\n",
      "epoch 920, loss 0.12570667266845703, L1 : 0.04454999044537544\n",
      "epoch 921, loss 0.11876529455184937, L1 : 0.04642360657453537\n",
      "epoch 922, loss 0.10932569205760956, L1 : 0.04014115780591965\n",
      "epoch 923, loss 0.10333473235368729, L1 : 0.03785404562950134\n",
      "epoch 924, loss 0.09755128622055054, L1 : 0.04033849388360977\n",
      "epoch 925, loss 0.10586424171924591, L1 : 0.03880394995212555\n",
      "epoch 926, loss 0.10271860659122467, L1 : 0.029762757942080498\n",
      "epoch 927, loss 0.11038660258054733, L1 : 0.03990267217159271\n",
      "epoch 928, loss 0.11397621035575867, L1 : 0.04414977505803108\n",
      "epoch 929, loss 0.1196226254105568, L1 : 0.04657527431845665\n",
      "epoch 930, loss 0.108909472823143, L1 : 0.04215850681066513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 931, loss 0.09901013970375061, L1 : 0.037492070347070694\n",
      "epoch 932, loss 0.11689233779907227, L1 : 0.040932804346084595\n",
      "epoch 933, loss 0.09781152009963989, L1 : 0.04131195321679115\n",
      "epoch 934, loss 0.09440305083990097, L1 : 0.03638917952775955\n",
      "epoch 935, loss 0.10433238744735718, L1 : 0.040520258247852325\n",
      "epoch 936, loss 0.1041012853384018, L1 : 0.03728873282670975\n",
      "epoch 937, loss 0.10688337683677673, L1 : 0.03808888420462608\n",
      "epoch 938, loss 0.10627707093954086, L1 : 0.04027731344103813\n",
      "epoch 939, loss 0.10453888028860092, L1 : 0.04154544696211815\n",
      "epoch 940, loss 0.10983450710773468, L1 : 0.03843660280108452\n",
      "epoch 941, loss 0.11065660417079926, L1 : 0.033649053424596786\n",
      "epoch 942, loss 0.1070106104016304, L1 : 0.038988273590803146\n",
      "epoch 943, loss 0.11717423796653748, L1 : 0.04182465746998787\n",
      "epoch 944, loss 0.1026415005326271, L1 : 0.04622901603579521\n",
      "epoch 945, loss 0.10141191631555557, L1 : 0.037308111786842346\n",
      "epoch 946, loss 0.11301898956298828, L1 : 0.0383673831820488\n",
      "epoch 947, loss 0.09781460464000702, L1 : 0.031009873375296593\n",
      "epoch 948, loss 0.12312959134578705, L1 : 0.0442923903465271\n",
      "epoch 949, loss 0.10931538045406342, L1 : 0.04110174626111984\n",
      "epoch 950, loss 0.1235179603099823, L1 : 0.04489345848560333\n",
      "epoch 951, loss 0.1158696711063385, L1 : 0.04648732393980026\n",
      "epoch 952, loss 0.11076033860445023, L1 : 0.04091453552246094\n",
      "epoch 953, loss 0.09083574265241623, L1 : 0.04000816494226456\n",
      "epoch 954, loss 0.11209357529878616, L1 : 0.039907556027173996\n",
      "epoch 955, loss 0.11879932880401611, L1 : 0.04813958704471588\n",
      "epoch 956, loss 0.10934330523014069, L1 : 0.04583246633410454\n",
      "epoch 957, loss 0.10015463829040527, L1 : 0.03639761731028557\n",
      "epoch 958, loss 0.10312680900096893, L1 : 0.03660646826028824\n",
      "epoch 959, loss 0.1086886003613472, L1 : 0.038392435759305954\n",
      "epoch 960, loss 0.09961505979299545, L1 : 0.04285868629813194\n",
      "epoch 961, loss 0.10209546983242035, L1 : 0.040208443999290466\n",
      "epoch 962, loss 0.0968962088227272, L1 : 0.040104832500219345\n",
      "epoch 963, loss 0.10701288282871246, L1 : 0.04399263486266136\n",
      "epoch 964, loss 0.11993302404880524, L1 : 0.04312913119792938\n",
      "epoch 965, loss 0.10396343469619751, L1 : 0.03613432124257088\n",
      "epoch 966, loss 0.11030973494052887, L1 : 0.03581641614437103\n",
      "epoch 967, loss 0.12662610411643982, L1 : 0.0471731498837471\n",
      "epoch 968, loss 0.10122577846050262, L1 : 0.038420844823122025\n",
      "epoch 969, loss 0.10996434092521667, L1 : 0.041138164699077606\n",
      "epoch 970, loss 0.09711218625307083, L1 : 0.038884054869413376\n",
      "epoch 971, loss 0.11417988687753677, L1 : 0.039628732949495316\n",
      "epoch 972, loss 0.10471973568201065, L1 : 0.04387335106730461\n",
      "epoch 973, loss 0.1056801974773407, L1 : 0.041067399084568024\n",
      "epoch 974, loss 0.10752805322408676, L1 : 0.044430602341890335\n",
      "epoch 975, loss 0.12463324517011642, L1 : 0.04257911816239357\n",
      "epoch 976, loss 0.09829195588827133, L1 : 0.032018616795539856\n",
      "epoch 977, loss 0.10181061923503876, L1 : 0.038599420338869095\n",
      "epoch 978, loss 0.10515666007995605, L1 : 0.04052991420030594\n",
      "epoch 979, loss 0.10651744902133942, L1 : 0.038471776992082596\n",
      "epoch 980, loss 0.10636318475008011, L1 : 0.03975805640220642\n",
      "epoch 981, loss 0.12663543224334717, L1 : 0.052257366478443146\n",
      "epoch 982, loss 0.11457104980945587, L1 : 0.03815620392560959\n",
      "epoch 983, loss 0.10342057794332504, L1 : 0.039494406431913376\n",
      "epoch 984, loss 0.10386314243078232, L1 : 0.04497615993022919\n",
      "epoch 985, loss 0.10148002207279205, L1 : 0.03721776604652405\n",
      "epoch 986, loss 0.11232586205005646, L1 : 0.04036874324083328\n",
      "epoch 987, loss 0.10785423219203949, L1 : 0.03885318711400032\n",
      "epoch 988, loss 0.11357370018959045, L1 : 0.03874405100941658\n",
      "epoch 989, loss 0.10784780979156494, L1 : 0.04591747000813484\n",
      "epoch 990, loss 0.10477660596370697, L1 : 0.03693176805973053\n",
      "epoch 991, loss 0.10144712030887604, L1 : 0.039320267736911774\n",
      "epoch 992, loss 0.09893817454576492, L1 : 0.036703698337078094\n",
      "epoch 993, loss 0.1109393835067749, L1 : 0.04483526945114136\n",
      "epoch 994, loss 0.11620145291090012, L1 : 0.04025014862418175\n",
      "epoch 995, loss 0.09329027682542801, L1 : 0.037897054105997086\n",
      "epoch 996, loss 0.1062375158071518, L1 : 0.04421320930123329\n",
      "epoch 997, loss 0.10856738686561584, L1 : 0.03680475056171417\n",
      "epoch 998, loss 0.105126291513443, L1 : 0.045584503561258316\n",
      "epoch 999, loss 0.11485795676708221, L1 : 0.036053694784641266\n",
      "epoch 1000, loss 0.11095015704631805, L1 : 0.03841886296868324\n",
      "Training finished in 724.97 for 1000.\n",
      "The final loss value is 0.11095015704631805\n"
     ]
    }
   ],
   "source": [
    "train_model(model, opt, scheduler, sampling_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "N = 41      # Points on plot grid\n",
    "\n",
    "times_to_plot = [0*T, 0.33*T, 0.66*T, T]\n",
    "tplot = np.linspace(T0, T, N)\n",
    "xplot = np.linspace(S1, S2, N)\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "i = 1\n",
    "for t in times_to_plot:\n",
    "    solution_plot = analytical_solution(t, xplot)\n",
    "\n",
    "    tt = t*np.ones_like(xplot.reshape(-1,1))\n",
    "    \n",
    "    tt_nn = torch.tensor(tt.reshape(-1,1), dtype=torch.float32).cuda()\n",
    "    xplot_nn = torch.tensor(xplot.reshape(-1,1), dtype=torch.float32).cuda()\n",
    "    nn_plot = model(tt_nn,xplot_nn).cpu()\n",
    "\n",
    "    plt.subplot(2,2,i)\n",
    "    plt.plot(xplot, solution_plot, 'b')\n",
    "    plt.plot(xplot, nn_plot.data.numpy(), 'r')\n",
    "\n",
    "    plt.ylim(-1.1, -0.2)\n",
    "    plt.xlabel(\"S\")\n",
    "    plt.ylabel(\"V\")\n",
    "    plt.title(\"t = %.2f\"%t, loc=\"left\")\n",
    "    i = i+1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = model(S1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        t1, x1, t2, x2, t3, x3, t4, x4 = sampler_space_time(NS_1, NS_2, NS_3) \n",
    "        alpha1, alpha2, alpha3, nu1, nu2, nu3, a_1, a_2, a_3, b_1, b_2, b_3 = sampler_parameters(NS_1, NS_2, NS_3)\n",
    "        \n",
    "        S1 = torch.cat((x1,t1,nu1, alpha1, a_1, b_1), 1)\n",
    "        S2 = torch.cat((x2,t2,nu2, alpha2, a_2, b_2), 1)\n",
    "        S3 = torch.cat((x3,t3,nu2, alpha2, a_2, b_2), 1)\n",
    "        S4 = torch.cat((x4,t4,nu3, alpha3, a_3, b_3), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    CI = g(S4[:,0], S4[:,4], S4[:,5])\n",
    "    L4 = torch.mean(torch.pow((model(S4)[:,0] - CI) ,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model(S4)[:,0] - CI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_t[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Loss term #1: PDE\n",
    "    U = model(S1)\n",
    "    DU = torch.autograd.grad(U.sum(), S1, create_graph=True, retain_graph=True)[0]\n",
    "    U_t = DU[:,1]\n",
    "    U_x = DU[:,0]\n",
    "    U_xx = torch.autograd.grad(U_x.sum(), S1, create_graph = True, retain_graph=True)[0][:,0]\n",
    "    \n",
    "    f = U_t - S1[:,2]*U_xx + S1[:,3] * U * U_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

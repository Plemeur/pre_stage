{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f53ee4027f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the architecture as seen in : https://arxiv.org/pdf/1811.08782.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGM_layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_feature, residual = False):\n",
    "        super(DGM_layer, self).__init__()\n",
    "        self.residual = residual\n",
    "        \n",
    "        self.Z = nn.Linear(out_feature,out_feature) ; self.UZ = nn.Linear(in_features,out_feature, bias=False)\n",
    "        self.G = nn.Linear(out_feature,out_feature) ; self.UG = nn.Linear(in_features,out_feature, bias=False)\n",
    "        self.R = nn.Linear(out_feature,out_feature) ; self.UR = nn.Linear(in_features,out_feature, bias=False)\n",
    "        self.H = nn.Linear(out_feature,out_feature) ; self.UH = nn.Linear(in_features,out_feature, bias=False)\n",
    "    \n",
    "\n",
    "    def forward(self, x, s):\n",
    "        z = torch.tanh(self.UZ(x)+self.Z(s))\n",
    "        g = torch.tanh(self.UG(x)+self.G(s))\n",
    "        r = torch.tanh(self.UR(x)+self.R(s))\n",
    "        h = torch.tanh(self.UH(x)+self.H(s*r))\n",
    "        return (1 - g) * h + z*s\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGM_net(nn.Module):\n",
    "    def __init__(self, in_dim,out_dim, n_layers, n_neurons, residual = False):\n",
    "        \"\"\" in_dim is number of cordinates + 1 \n",
    "            out_dim is the number of output\n",
    "            n_layers and n_neurons are pretty self explanatory\n",
    "            make residual = true for identity between each DGM layers\n",
    "        \"\"\"\n",
    "        super(DGM_net, self).__init__()\n",
    "        self.in_dim = in_dim ; self.out_dim = out_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.residual = residual\n",
    "\n",
    "        self.first_layer = nn.Linear(in_dim, n_neurons)\n",
    "        \n",
    "        self.dgm_layers = nn.ModuleList([DGM_layer(self.in_dim, self.n_neurons,\n",
    "                                                       self.residual) for i in range(self.n_layers)])\n",
    "        self.final_layer = nn.Linear(n_neurons,out_dim)\n",
    "    \n",
    "    def forward(self,X,t):\n",
    "        x = torch.cat((X,t),1)\n",
    "        s = torch.tanh(self.first_layer(x))\n",
    "        for i,dgm_layer in enumerate(self.dgm_layers):\n",
    "            s = dgm_layer(x, s)\n",
    "        \n",
    "        return  self.final_layer(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 400 #thermal conductivity of copper\n",
    "stefanBoltz = 5.670373e-8 # stefan-boltzman cst\n",
    "h = 1 #plate size\n",
    "Tb = 1000 #temperature of the bottom\n",
    "epsilon=0.5 # emissivity\n",
    "rho = 8960 # Copper density\n",
    "Cp=386 #specific heat\n",
    "Ta = 300 #ambient temperature\n",
    "tz = 0.01 # plate thickness\n",
    "\n",
    "#for ease of use\n",
    "a = rho*Cp*tz\n",
    "b = -k*tz\n",
    "c = 2*h\n",
    "d = 2*stefanBoltz*epsilon\n",
    "\n",
    "\n",
    "# Time limits\n",
    "T0 = 0.0 + 1e-10    # Initial time\n",
    "T  = 5000.0            # Terminal time\n",
    "\n",
    "# Space limits\n",
    "S1 = 0.0 + 1e-10    # Low boundary\n",
    "S2 = 1              # High boundary\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "steps_per_sample = 10\n",
    "sampling_stages = 800\n",
    "\n",
    "# Number of samples\n",
    "NS_1 = 500  #domain all t\n",
    "NS_2 = 100   #dirichlet\n",
    "NS_3 = 300   #neumann\n",
    "NS_4 = 100   # initial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler(N1, N2, N3, N4):\n",
    "    # Sampler #1: PDE domain\n",
    "    t1 = np.random.uniform(low=T0 - 0.5*(T - T0)  ,\n",
    "                           high=T,\n",
    "                           size=[N1,1])\n",
    "    s1 = np.random.uniform(low=S1 - (S2 - S1)*0.5,\n",
    "                           high=S2 + (S2 - S1)*0.5,\n",
    "                           size=[N1,2])\n",
    "\n",
    "    # Sampler #2: Dirichlet\n",
    "    t2 = np.random.uniform(low=T0 - 0.5*(T - T0) , high=T,\n",
    "                           size=[N2,1])\n",
    "    s2 = np.random.uniform(low=S1 , high=S2  ,\n",
    "                           size=[N2,2])\n",
    "    s2[:,1]=0 #y=0\n",
    "    \n",
    "    # Sampler #3: Neumann\n",
    "    t3 = np.random.uniform(low=T0 - 0.5*(T - T0) , high=T,\n",
    "                           size=[N3,1])\n",
    "    s3 = np.random.uniform(low=S1 - (S2 - S1)*0.5 , high=S2 + (S2 - S1)*0.5 ,\n",
    "                           size=[N3,2])\n",
    "    s3[:100,0]=0  #x=0\n",
    "    s3[100:200,0]= 1 #x=1\n",
    "    s3[200:,1]= 1 #y=1\n",
    "    \n",
    "    \n",
    "    # Sampler #4: initial/terminal condition\n",
    "    t4 = 0 * np.ones((N4,1)) #Initial condition\n",
    "    s4 = np.random.uniform(low=S1, high=S2,\n",
    "                           size=[N4,2])\n",
    "    \n",
    "    t1=torch.tensor(t1, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    s1=torch.tensor(s1, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    t2=torch.tensor(t2, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    s2=torch.tensor(s2, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    t3=torch.tensor(t3, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    s3=torch.tensor(s3, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    t4=torch.tensor(t4, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    s4=torch.tensor(s4, dtype=torch.float32, requires_grad=True).cuda()\n",
    "    \n",
    "    return (t1, s1, t2, s2, t3, s3, t4, s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(model, t1, x1, t2, x2, t3, x3, t4, x4):\n",
    "    # Loss term #1: PDE\n",
    "    U = model(t1, x1)\n",
    "    U_t = torch.autograd.grad(U.sum(), t1, create_graph=True, retain_graph=True)[0]\n",
    "    dU = torch.autograd.grad(U.sum(), x1, create_graph=True, retain_graph=True)[0]\n",
    "    U_xx = torch.autograd.grad(dU[:,0].sum(), x1, create_graph = True, retain_graph=True)[0][:,0]\n",
    "    U_yy = torch.autograd.grad(dU[:,1].sum(), x1, create_graph = True, retain_graph=True)[0][:,1]\n",
    "\n",
    "    f = a*U_t + b*(U_xx + U_yy) + c*(U-Ta) + d*(U**4-Ta**4)\n",
    "    L1 = torch.mean(torch.pow(f,2))\n",
    "\n",
    "    # Loss term #2: Dirichlet boundary condition\n",
    "    L2 = torch.mean(torch.pow(model(t2,x2)[:,0]-Tb,2))\n",
    "   \n",
    "    # Loss term #3: initial/terminal condition\n",
    "    Un = model(t3,x3)\n",
    "    dUn =  torch.autograd.grad(Un.sum(), x3, create_graph=True, retain_graph=True)[0]\n",
    "    #GOT TO MAKE A VAR FOR NUMBER OF POINT PER SIDE TO HAVE A CORRECT IMPLEMENTATION !!!!!!\n",
    "    dUn_normal = torch.cat((dUn[:200,0], dUn[200:,1]))\n",
    "    L3 = torch.mean(torch.pow(dUn_normal,2))\n",
    "    #L3 = torch.tensor(0.0)\n",
    "\n",
    "    \n",
    "    L4 = torch.mean(torch.pow((model(x4,t4)[:,0]-Ta),2))\n",
    "    \n",
    "\n",
    "    \n",
    "    return L1, L2, L3, L4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model init and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DGM_net(3,1,8,200)\n",
    "model.cuda()\n",
    "opt = torch.optim.Adam(model.parameters(),lr = 0.005)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(opt,gamma=0.99)\n",
    "\n",
    "# Training parameters\n",
    "steps_per_sample = 20\n",
    "sampling_stages = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, num_epochs=100):\n",
    "    since = time.time()\n",
    "    model.train()\n",
    "  # Set model to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        t1, x1, t2, x2, t3, x3, t4, x4 = sampler(NS_1, NS_2, NS_3, NS_4) #\n",
    "        scheduler.step()\n",
    "\n",
    "        for _ in range(steps_per_sample) :\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # forward\n",
    "            L1, L2, L3, L4 = Loss(model, t1, x1, t2, x2, t3, x3, t4, x4)\n",
    "    \n",
    "            loss = L1 + L2 + L4  \n",
    "            # backward + optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        epoch +=1 \n",
    "        if epoch % (num_epochs//num_epochs) == 0: print(f'epoch {epoch}, loss {loss.data}, L1 : {L1.data}, L2 : {L2.data}, L3 : {L3.data}, L4 : {L4.data}')\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f\"Training finished in {time_elapsed:.2f} for {num_epochs}.\")\n",
    "    print(f\"The final loss value is {loss.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1988192.875, L1 : 933170.75, L2 : 989962.875, L3 : 0.06737013161182404, L4 : 65059.30859375\n",
      "epoch 2, loss 2045878.75, L1 : 968860.125, L2 : 991026.375, L3 : 0.02315964549779892, L4 : 85992.2578125\n",
      "epoch 3, loss 2034429.875, L1 : 955527.5625, L2 : 991211.0, L3 : 0.002334861783310771, L4 : 87691.359375\n",
      "epoch 4, loss 2258178.5, L1 : 1175156.875, L2 : 992487.0, L3 : 0.0019701167475432158, L4 : 90534.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8b1eae9c51be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_stages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-673abcc32e1f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mL2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mL4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, opt, scheduler, sampling_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1, x1, t2, x2, t3, x3, t4, x4 = sampler(NS_1, NS_2, NS_3, NS_4) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6263, 1.0000],\n",
       "        [0.1139, 1.0000],\n",
       "        [0.7631, 1.0000],\n",
       "        [0.6531, 1.0000],\n",
       "        [0.2642, 1.0000],\n",
       "        [0.0770, 1.0000],\n",
       "        [0.4420, 1.0000],\n",
       "        [0.6800, 1.0000],\n",
       "        [0.8761, 1.0000],\n",
       "        [0.7321, 1.0000],\n",
       "        [0.9458, 1.0000],\n",
       "        [0.1331, 1.0000],\n",
       "        [0.0976, 1.0000],\n",
       "        [0.6534, 1.0000],\n",
       "        [0.1483, 1.0000],\n",
       "        [0.7799, 1.0000],\n",
       "        [0.5299, 1.0000],\n",
       "        [0.6721, 1.0000],\n",
       "        [0.4372, 1.0000],\n",
       "        [0.0093, 1.0000],\n",
       "        [0.8439, 1.0000],\n",
       "        [0.0209, 1.0000],\n",
       "        [0.0079, 1.0000],\n",
       "        [0.1766, 1.0000],\n",
       "        [0.8470, 1.0000],\n",
       "        [0.2923, 1.0000],\n",
       "        [0.2866, 1.0000],\n",
       "        [0.4132, 1.0000],\n",
       "        [0.6335, 1.0000],\n",
       "        [0.4994, 1.0000],\n",
       "        [0.1232, 1.0000],\n",
       "        [0.4166, 1.0000],\n",
       "        [0.8696, 1.0000],\n",
       "        [0.9125, 1.0000],\n",
       "        [0.3690, 1.0000],\n",
       "        [0.7375, 1.0000],\n",
       "        [0.6659, 1.0000],\n",
       "        [0.1369, 1.0000],\n",
       "        [0.0224, 1.0000],\n",
       "        [0.7888, 1.0000],\n",
       "        [0.9381, 1.0000],\n",
       "        [0.1173, 1.0000],\n",
       "        [0.0229, 1.0000],\n",
       "        [0.0289, 1.0000],\n",
       "        [0.5604, 1.0000],\n",
       "        [0.6384, 1.0000],\n",
       "        [0.6161, 1.0000],\n",
       "        [0.4255, 1.0000],\n",
       "        [0.5063, 1.0000],\n",
       "        [0.9740, 1.0000],\n",
       "        [0.1047, 1.0000],\n",
       "        [0.9185, 1.0000],\n",
       "        [0.0661, 1.0000],\n",
       "        [0.1302, 1.0000],\n",
       "        [0.6649, 1.0000],\n",
       "        [0.7554, 1.0000],\n",
       "        [0.4517, 1.0000],\n",
       "        [0.4393, 1.0000],\n",
       "        [0.8810, 1.0000],\n",
       "        [0.1213, 1.0000],\n",
       "        [0.4777, 1.0000],\n",
       "        [0.8078, 1.0000],\n",
       "        [0.6337, 1.0000],\n",
       "        [0.6562, 1.0000],\n",
       "        [0.2370, 1.0000],\n",
       "        [0.7418, 1.0000],\n",
       "        [0.5575, 1.0000],\n",
       "        [0.8209, 1.0000],\n",
       "        [0.3212, 1.0000],\n",
       "        [0.7350, 1.0000],\n",
       "        [0.2300, 1.0000],\n",
       "        [0.4417, 1.0000],\n",
       "        [0.9845, 1.0000],\n",
       "        [0.5332, 1.0000],\n",
       "        [0.7593, 1.0000],\n",
       "        [0.4625, 1.0000],\n",
       "        [0.2378, 1.0000],\n",
       "        [0.3717, 1.0000],\n",
       "        [0.9329, 1.0000],\n",
       "        [0.0255, 1.0000],\n",
       "        [0.4542, 1.0000],\n",
       "        [0.9930, 1.0000],\n",
       "        [0.8544, 1.0000],\n",
       "        [0.1214, 1.0000],\n",
       "        [0.6866, 1.0000],\n",
       "        [0.0895, 1.0000],\n",
       "        [0.8904, 1.0000],\n",
       "        [0.5629, 1.0000],\n",
       "        [0.1453, 1.0000],\n",
       "        [0.9238, 1.0000],\n",
       "        [0.3029, 1.0000],\n",
       "        [0.0762, 1.0000],\n",
       "        [0.4074, 1.0000],\n",
       "        [0.8099, 1.0000],\n",
       "        [0.3096, 1.0000],\n",
       "        [0.6222, 1.0000],\n",
       "        [0.5129, 1.0000],\n",
       "        [0.6778, 1.0000],\n",
       "        [0.4586, 1.0000],\n",
       "        [0.4677, 1.0000]], device='cuda:0', grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.grad.data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xplot.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

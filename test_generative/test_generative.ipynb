{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from autograd import grad, jacobian\n",
    "from math import pi #to get the definition of pi\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Laplacian.csv', header=None) #let's import the grid\n",
    "X = torch.tensor(data.values[:, 0:2], requires_grad=True).to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still working on the laplacian equation $-\\Delta u(x,y)= f(x,y)$.\n",
    "\n",
    "According to this paper https://arxiv.org/pdf/physics/9705023.pdf, we have to write the output of our neural network as :\n",
    "\n",
    "$$ \\Psi_t(x,y) = A(x,y) + x(1-x)y(1-y)N(x,y,p)$$\n",
    "\n",
    "were A is the part that takes care of the boundary conditions and p is the weight matrix of the neural net.\n",
    "\n",
    "In the end, the loss function is $ E[p] = \\sum_i \\{\\frac{\\partial}{\\partial x^2}\\Psi(x_i,y_i) + \\frac{\\partial}{\\partial y^2}\\Psi(x_i,y_i) - f(x_i, y_i)\\}^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's deal with the boundary conditions and the f function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = pi/2 ; beta = pi # We use those to be able to compare to an exact solution\n",
    "def A(X) : return torch.sin(alpha*X[:,0])*torch.cos(beta*X[:,1])\n",
    "def f(X) : return (alpha**2 + beta**2)*torch.sin(alpha*X[:,0])*torch.cos(beta*X[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can write the Psi function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Psi(X, N) : return A(X) + X[:,0]*(1-X[:,0])*X[:,1]*(1-X[:,1]) * N.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play around to see if torch.autograd.grad is giving us what we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn(2,1, requires_grad=True)\n",
    "N = X@W; \n",
    "output = Psi(X, N).sum()\n",
    "jac = torch.autograd.grad(output, X, create_graph=True); #jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (torch.autograd.grad(jac[0][:,0].sum(),X, create_graph=True)[0][:,1] -\n",
    "#torch.autograd.grad(jac[0][:,1].sum(),X, create_graph=True)[0][:,0]) \n",
    "#just to check if cross derivative are equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we have an idea about how to compute Jacobian and hessian matrices, let's build our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(X,N) :\n",
    "    N = model(X)\n",
    "    output = Psi(X, N).sum()\n",
    "    jac = torch.autograd.grad(output, X, create_graph=True)\n",
    "    hes_x = torch.autograd.grad(jac[0][:,0].sum(),X, create_graph=True)[0][:,0]\n",
    "    hes_y = torch.autograd.grad(jac[0][:,1].sum(),X, create_graph=True)[0][:,1]\n",
    "    \n",
    "    loss =(((hes_x + hes_y) - f(X))**2).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    # Initialize the layers\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(2, 64, bias=True) #multipliyin the input by some weigth\n",
    "        self.linear2 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = torch.sigmoid(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNet() #the network we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr=0.001):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr, weight_decay=0.01) #gradient descent\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch +=1\n",
    "        opt.zero_grad()\n",
    "        loss = Loss(X,N)\n",
    "        loss.backward()# back props\n",
    "        opt.step()# update the parameters\n",
    "        if epoch % (epochs//10) == 0: print('epoch {}, loss {}'.format(epoch, loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 10.731881141662598\n",
      "epoch 200, loss 8.073054313659668\n",
      "epoch 300, loss 6.1805291175842285\n",
      "epoch 400, loss 5.4158854484558105\n",
      "epoch 500, loss 5.155508518218994\n",
      "epoch 600, loss 5.121185779571533\n",
      "epoch 700, loss 5.086486339569092\n",
      "epoch 800, loss 5.168694019317627\n",
      "epoch 900, loss 5.051408290863037\n",
      "epoch 1000, loss 5.059935569763184\n"
     ]
    }
   ],
   "source": [
    "fit(1000,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
